{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Style Transfer in Google Colab\n",
        "\n",
        "This project implements **Neural Style Transfer (NST)** using **Pytorch** in **Google Colab** for ease of execution without requiring local GPU resources. It is based on the research paper:\n",
        "\n",
        "[**Image Style Transfer Using Convolutional Neural Networks**](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)\n",
        "by *Leon A. Gatys*\n",
        "\n",
        "The implementation is done using **PyTorch** and follows insights from multiple sources, particularly the YouTube tutorials by [ **Aleksa Gordić**.](https://www.youtube.com/playlist?list=PLBoQnSflObcmbfshq9oNs41vODgXG-608)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HfUDdKj1u9mt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries required"
      ],
      "metadata": {
        "id": "1miT6vgTkFFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "import cv2 as cv\n",
        "\n",
        "from collections import namedtuple\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Adam, LBFGS"
      ],
      "metadata": {
        "id": "-OzS5SvwdSci"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "Sl88mDfMcDK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vgg16(torch.nn.Module):\n",
        "\n",
        "    \"\"\"Only those layers are exposed which have already proven to work nicely.\"\"\"\n",
        "    def __init__(self, requires_grad=False, show_progress=False):\n",
        "        super().__init__()\n",
        "        vgg_pretrained_features = models.vgg16(pretrained=True, progress=show_progress).features\n",
        "        self.layer_names = ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3']\n",
        "        self.content_feature_maps_index = 1\n",
        "        self.style_feature_maps_indices = list(range(len(self.layer_names)))  # all layers used for style representation\n",
        "\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "        for x in range(4):\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(4, 9):\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(9, 16):\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(16, 23):\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.slice1(x)\n",
        "        relu1_2 = x\n",
        "        x = self.slice2(x)\n",
        "        relu2_2 = x\n",
        "        x = self.slice3(x)\n",
        "        relu3_3 = x\n",
        "        x = self.slice4(x)\n",
        "        relu4_3 = x\n",
        "        vgg_outputs = namedtuple(\"VggOutputs\", self.layer_names)\n",
        "        out = vgg_outputs(relu1_2, relu2_2, relu3_3, relu4_3)\n",
        "        return out"
      ],
      "metadata": {
        "id": "5Ece7VZOcTHK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vgg16Experimental(torch.nn.Module):\n",
        "\n",
        "    \"\"\"Everything exposed so you can play with different combinations for style and content representation\"\"\"\n",
        "    def __init__(self, requires_grad=False, show_progress=False):\n",
        "        super().__init__()\n",
        "        vgg_pretrained_features = models.vgg16(pretrained=True, progress=show_progress).features\n",
        "        self.layer_names = ['relu1_1', 'relu2_1', 'relu2_2', 'relu3_1', 'relu3_2', 'relu4_1', 'relu4_3', 'relu5_1']\n",
        "        self.content_feature_maps_index = 4\n",
        "        self.style_feature_maps_indices = list(range(len(self.layer_names)))  # all layers used for style representation\n",
        "\n",
        "        self.conv1_1 = vgg_pretrained_features[0]\n",
        "        self.relu1_1 = vgg_pretrained_features[1]\n",
        "        self.conv1_2 = vgg_pretrained_features[2]\n",
        "        self.relu1_2 = vgg_pretrained_features[3]\n",
        "        self.max_pooling1 = vgg_pretrained_features[4]\n",
        "        self.conv2_1 = vgg_pretrained_features[5]\n",
        "        self.relu2_1 = vgg_pretrained_features[6]\n",
        "        self.conv2_2 = vgg_pretrained_features[7]\n",
        "        self.relu2_2 = vgg_pretrained_features[8]\n",
        "        self.max_pooling2 = vgg_pretrained_features[9]\n",
        "        self.conv3_1 = vgg_pretrained_features[10]\n",
        "        self.relu3_1 = vgg_pretrained_features[11]\n",
        "        self.conv3_2 = vgg_pretrained_features[12]\n",
        "        self.relu3_2 = vgg_pretrained_features[13]\n",
        "        self.conv3_3 = vgg_pretrained_features[14]\n",
        "        self.relu3_3 = vgg_pretrained_features[15]\n",
        "        self.max_pooling3 = vgg_pretrained_features[16]\n",
        "        self.conv4_1 = vgg_pretrained_features[17]\n",
        "        self.relu4_1 = vgg_pretrained_features[18]\n",
        "        self.conv4_2 = vgg_pretrained_features[19]\n",
        "        self.relu4_2 = vgg_pretrained_features[20]\n",
        "        self.conv4_3 = vgg_pretrained_features[21]\n",
        "        self.relu4_3 = vgg_pretrained_features[22]\n",
        "        self.max_pooling4 = vgg_pretrained_features[23]\n",
        "        self.conv5_1 = vgg_pretrained_features[24]\n",
        "        self.relu5_1 = vgg_pretrained_features[25]\n",
        "        self.conv5_2 = vgg_pretrained_features[26]\n",
        "        self.relu5_2 = vgg_pretrained_features[27]\n",
        "        self.conv5_3 = vgg_pretrained_features[28]\n",
        "        self.relu5_3 = vgg_pretrained_features[29]\n",
        "        self.max_pooling5 = vgg_pretrained_features[30]\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1_1(x)\n",
        "        conv1_1 = x\n",
        "        x = self.relu1_1(x)\n",
        "        relu1_1 = x\n",
        "        x = self.conv1_2(x)\n",
        "        conv1_2 = x\n",
        "        x = self.relu1_2(x)\n",
        "        relu1_2 = x\n",
        "        x = self.max_pooling1(x)\n",
        "        x = self.conv2_1(x)\n",
        "        conv2_1 = x\n",
        "        x = self.relu2_1(x)\n",
        "        relu2_1 = x\n",
        "        x = self.conv2_2(x)\n",
        "        conv2_2 = x\n",
        "        x = self.relu2_2(x)\n",
        "        relu2_2 = x\n",
        "        x = self.max_pooling2(x)\n",
        "        x = self.conv3_1(x)\n",
        "        conv3_1 = x\n",
        "        x = self.relu3_1(x)\n",
        "        relu3_1 = x\n",
        "        x = self.conv3_2(x)\n",
        "        conv3_2 = x\n",
        "        x = self.relu3_2(x)\n",
        "        relu3_2 = x\n",
        "        x = self.conv3_3(x)\n",
        "        conv3_3 = x\n",
        "        x = self.relu3_3(x)\n",
        "        relu3_3 = x\n",
        "        x = self.max_pooling3(x)\n",
        "        x = self.conv4_1(x)\n",
        "        conv4_1 = x\n",
        "        x = self.relu4_1(x)\n",
        "        relu4_1 = x\n",
        "        x = self.conv4_2(x)\n",
        "        conv4_2 = x\n",
        "        x = self.relu4_2(x)\n",
        "        relu4_2 = x\n",
        "        x = self.conv4_3(x)\n",
        "        conv4_3 = x\n",
        "        x = self.relu4_3(x)\n",
        "        relu4_3 = x\n",
        "        x = self.max_pooling4(x)\n",
        "        x = self.conv5_1(x)\n",
        "        conv5_1 = x\n",
        "        x = self.relu5_1(x)\n",
        "        relu5_1 = x\n",
        "        x = self.conv5_2(x)\n",
        "        conv5_2 = x\n",
        "        x = self.relu5_2(x)\n",
        "        relu5_2 = x\n",
        "        x = self.conv5_3(x)\n",
        "        conv5_3 = x\n",
        "        x = self.relu5_3(x)\n",
        "        relu5_3 = x\n",
        "        x = self.max_pooling5(x)\n",
        "\n",
        "\n",
        "        vgg_outputs = namedtuple(\"VggOutputs\", self.layer_names)\n",
        "        out = vgg_outputs(relu1_1, relu2_1, relu2_2, relu3_1, relu3_2, relu4_1, relu4_3, relu5_1)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "NMRRfPxHdPJS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vgg19(torch.nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Used in the original NST paper, only those layers are exposed which were used in the original paper\n",
        "\n",
        "    'conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1' were used for style representation\n",
        "    'conv4_2' was used for content representation (although they did some experiments with conv2_2 and conv5_2)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, requires_grad=False, show_progress=False, use_relu=True):\n",
        "        super().__init__()\n",
        "        vgg_pretrained_features = models.vgg19(pretrained=True, progress=show_progress).features\n",
        "        if use_relu:  # use relu or as in original paper conv layers\n",
        "            self.layer_names = ['relu1_1', 'relu2_1', 'relu3_1', 'relu4_1', 'conv4_2', 'relu5_1']\n",
        "            self.offset = 1\n",
        "        else:\n",
        "            self.layer_names = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv4_2', 'conv5_1']\n",
        "            self.offset = 0\n",
        "        self.content_feature_maps_index = 4  # conv4_2\n",
        "        # all layers used for style representation except conv4_2\n",
        "        self.style_feature_maps_indices = list(range(len(self.layer_names)))\n",
        "        self.style_feature_maps_indices.remove(4)  # conv4_2\n",
        "\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "        self.slice5 = torch.nn.Sequential()\n",
        "        self.slice6 = torch.nn.Sequential()\n",
        "        for x in range(1+self.offset):\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(1+self.offset, 6+self.offset):\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(6+self.offset, 11+self.offset):\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(11+self.offset, 20+self.offset):\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(20+self.offset, 22):\n",
        "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(22, 29++self.offset):\n",
        "            self.slice6.add_module(str(x), vgg_pretrained_features[x])\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.slice1(x)\n",
        "        layer1_1 = x\n",
        "        x = self.slice2(x)\n",
        "        layer2_1 = x\n",
        "        x = self.slice3(x)\n",
        "        layer3_1 = x\n",
        "        x = self.slice4(x)\n",
        "        layer4_1 = x\n",
        "        x = self.slice5(x)\n",
        "        conv4_2 = x\n",
        "        x = self.slice6(x)\n",
        "        layer5_1 = x\n",
        "        vgg_outputs = namedtuple(\"VggOutputs\", self.layer_names)\n",
        "        out = vgg_outputs(layer1_1, layer2_1, layer3_1, layer4_1, conv4_2, layer5_1)\n",
        "        return out"
      ],
      "metadata": {
        "id": "FPTYT3GGdPA-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "mIjCcNHFksSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGENET_MEAN_255 = [123.675, 116.28, 103.53]\n",
        "IMAGENET_STD_NEUTRAL = [1, 1, 1]"
      ],
      "metadata": {
        "id": "9WpJGxEupYpw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(img_path, target_shape=None):\n",
        "    if not os.path.exists(img_path):\n",
        "        raise Exception(f'Path does not exist: {img_path}')\n",
        "    img = cv.imread(img_path)[:, :, ::-1]\n",
        "\n",
        "    if target_shape is not None:\n",
        "        if isinstance(target_shape, int) and target_shape != -1:\n",
        "            current_height, current_width = img.shape[:2]\n",
        "            new_height = target_shape\n",
        "            new_width = int(current_width * (new_height / current_height))\n",
        "            img = cv.resize(img, (new_width, new_height), interpolation=cv.INTER_CUBIC)\n",
        "        else:\n",
        "            img = cv.resize(img, (target_shape[1], target_shape[0]), interpolation=cv.INTER_CUBIC)\n",
        "\n",
        "\n",
        "    img = img.astype(np.float32)\n",
        "    img /= 255.0\n",
        "    return img\n",
        "\n",
        "\n",
        "def prepare_img(img_path, target_shape, device):\n",
        "    img = load_image(img_path, target_shape=target_shape)\n",
        "\n",
        "\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.mul(255)),\n",
        "        transforms.Normalize(mean=IMAGENET_MEAN_255, std=IMAGENET_STD_NEUTRAL)\n",
        "    ])\n",
        "\n",
        "    img = transform(img).to(device).unsqueeze(0)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def save_image(img, img_path):\n",
        "    if len(img.shape) == 2:\n",
        "        img = np.stack((img,) * 3, axis=-1)\n",
        "    cv.imwrite(img_path, img[:, :, ::-1])\n",
        "\n",
        "\n",
        "def generate_out_img_name(config):\n",
        "    prefix = os.path.basename(config['content_img_name']).split('.')[0] + '_' + os.path.basename(config['style_img_name']).split('.')[0]\n",
        "\n",
        "    if 'reconstruct_script' in config:\n",
        "        suffix = f'_o_{config[\"optimizer\"]}_h_{str(config[\"height\"])}_m_{config[\"model\"]}{config[\"img_format\"][1]}'\n",
        "    else:\n",
        "        suffix = f'_o_{config[\"optimizer\"]}_i_{config[\"init_method\"]}_h_{str(config[\"height\"])}_m_{config[\"model\"]}_cw_{config[\"content_weight\"]}_sw_{config[\"style_weight\"]}_tv_{config[\"tv_weight\"]}{config[\"img_format\"][1]}'\n",
        "    return prefix + suffix"
      ],
      "metadata": {
        "id": "PWN_B3i5jr9y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_and_maybe_display(optimizing_img, dump_path, config, img_id, num_of_iterations, should_display=False):\n",
        "    saving_freq = config['saving_freq']\n",
        "    out_img = optimizing_img.squeeze(axis=0).to('cpu').detach().numpy()\n",
        "    out_img = np.moveaxis(out_img, 0, 2)\n",
        "\n",
        "    if img_id == num_of_iterations-1 or (saving_freq > 0 and img_id % saving_freq == 0):\n",
        "        img_format = config['img_format']\n",
        "        out_img_name = str(img_id).zfill(img_format[0]) + img_format[1] if saving_freq != -1 else generate_out_img_name(config)\n",
        "        dump_img = np.copy(out_img)\n",
        "        dump_img += np.array(IMAGENET_MEAN_255).reshape((1, 1, 3))\n",
        "        dump_img = np.clip(dump_img, 0, 255).astype('uint8')\n",
        "        cv.imwrite(os.path.join(dump_path, out_img_name), dump_img[:, :, ::-1])\n",
        "\n",
        "    if should_display:\n",
        "        plt.imshow(np.uint8(get_uint8_range(out_img)))\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "z3g6l3aPqT0k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_uint8_range(x):\n",
        "    if isinstance(x, np.ndarray):\n",
        "        x -= np.min(x)\n",
        "        x /= np.max(x)\n",
        "        x *= 255\n",
        "        return x\n",
        "    else:\n",
        "        raise ValueError(f'Expected numpy array got {type(x)}')"
      ],
      "metadata": {
        "id": "atkYUkdGqRpo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_model(model, device):\n",
        "\n",
        "    #  only tuning optimizing_img's pixels\n",
        "    experimental = False\n",
        "    if model == 'vgg16':\n",
        "        if experimental:\n",
        "\n",
        "            # much more flexible for experimenting with different style representations\n",
        "            model = Vgg16Experimental(requires_grad=False, show_progress=True)\n",
        "        else:\n",
        "            model = Vgg16(requires_grad=False, show_progress=True)\n",
        "    elif model == 'vgg19':\n",
        "        model = Vgg19(requires_grad=False, show_progress=True)\n",
        "    else:\n",
        "        raise ValueError(f'{model} not supported.')\n",
        "\n",
        "    content_feature_maps_index = model.content_feature_maps_index\n",
        "    style_feature_maps_indices = model.style_feature_maps_indices\n",
        "    layer_names = model.layer_names\n",
        "\n",
        "    content_fms_index_name = (content_feature_maps_index, layer_names[content_feature_maps_index])\n",
        "    style_fms_indices_names = (style_feature_maps_indices, layer_names)\n",
        "    return model.to(device).eval(), content_fms_index_name, style_fms_indices_names"
      ],
      "metadata": {
        "id": "4dHkWHBeqPpP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gram_matrix(x, should_normalize=True):\n",
        "    (b, ch, h, w) = x.size()\n",
        "    features = x.view(b, ch, w * h)\n",
        "    features_t = features.transpose(1, 2)\n",
        "    gram = features.bmm(features_t)\n",
        "    if should_normalize:\n",
        "        gram /= ch * h * w\n",
        "    return gram"
      ],
      "metadata": {
        "id": "UTv74808qLGX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def total_variation(y):\n",
        "    return torch.sum(torch.abs(y[:, :, :, :-1] - y[:, :, :, 1:])) + \\\n",
        "           torch.sum(torch.abs(y[:, :, :-1, :] - y[:, :, 1:, :]))"
      ],
      "metadata": {
        "id": "FcO3WZRiqIiQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creates required directories\n",
        "dirs = [\n",
        "    \"data\",\n",
        "    \"data/content-images\",\n",
        "    \"data/style-images\",\n",
        "    \"data/output-images\"\n",
        "]\n",
        "\n",
        "for d in dirs:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "print(\"Directories created successfully!\")\n",
        "\n",
        "\n",
        "# The content and style images are need to be added manually (for colab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66smhWxPhWzk",
        "outputId": "64207325-83fa-4fe8-e694-108fcd72f63c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directories created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NST"
      ],
      "metadata": {
        "id": "1z2_Vb6z5GQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_loss(neural_net, optimizing_img, target_representations, content_feature_maps_index, style_feature_maps_indices, config):\n",
        "    target_content_representation = target_representations[0]\n",
        "    target_style_representation = target_representations[1]\n",
        "\n",
        "    current_set_of_feature_maps = neural_net(optimizing_img)\n",
        "\n",
        "    current_content_representation = current_set_of_feature_maps[content_feature_maps_index].squeeze(axis=0)\n",
        "    content_loss = torch.nn.MSELoss(reduction='mean')(target_content_representation, current_content_representation)\n",
        "\n",
        "    style_loss = 0.0\n",
        "    current_style_representation = [gram_matrix(x) for cnt, x in enumerate(current_set_of_feature_maps) if cnt in style_feature_maps_indices]\n",
        "    for gram_gt, gram_hat in zip(target_style_representation, current_style_representation):\n",
        "        style_loss += torch.nn.MSELoss(reduction='sum')(gram_gt[0], gram_hat[0])\n",
        "    style_loss /= len(target_style_representation)\n",
        "\n",
        "    tv_loss = total_variation(optimizing_img)\n",
        "\n",
        "    total_loss = config['content_weight'] * content_loss + config['style_weight'] * style_loss + config['tv_weight'] * tv_loss\n",
        "\n",
        "    return total_loss, content_loss, style_loss, tv_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "d0K9n3r5evs_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_tuning_step(neural_net, optimizer, target_representations, content_feature_maps_index, style_feature_maps_indices, config):\n",
        "\n",
        "    def tuning_step(optimizing_img):\n",
        "        total_loss, content_loss, style_loss, tv_loss = build_loss(neural_net, optimizing_img, target_representations, content_feature_maps_index, style_feature_maps_indices, config)\n",
        "\n",
        "        total_loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        return total_loss, content_loss, style_loss, tv_loss\n",
        "\n",
        "\n",
        "    return tuning_step"
      ],
      "metadata": {
        "id": "OQBoPgJkeyMm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RAY6rtp0k-dV"
      },
      "outputs": [],
      "source": [
        "def neural_style_transfer(config):\n",
        "    content_img_path = os.path.join(config['content_images_dir'], config['content_img_name'])\n",
        "    style_img_path = os.path.join(config['style_images_dir'], config['style_img_name'])\n",
        "\n",
        "    out_dir_name = 'combined_' + os.path.split(content_img_path)[1].split('.')[0] + '_' + os.path.split(style_img_path)[1].split('.')[0]\n",
        "    dump_path = os.path.join(config['output_img_dir'], out_dir_name)\n",
        "    os.makedirs(dump_path, exist_ok=True)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    content_img = prepare_img(content_img_path, config['height'], device)\n",
        "    style_img = prepare_img(style_img_path, config['height'], device)\n",
        "\n",
        "    if config['init_method'] == 'random':\n",
        "\n",
        "        # white_noise_img = np.random.uniform(-90., 90., content_img.shape).astype(np.float32)\n",
        "        gaussian_noise_img = np.random.normal(loc=0, scale=90., size=content_img.shape).astype(np.float32)\n",
        "        init_img = torch.from_numpy(gaussian_noise_img).float().to(device)\n",
        "    elif config['init_method'] == 'content':\n",
        "        init_img = content_img\n",
        "    else:\n",
        "\n",
        "\n",
        "        style_img_resized = prepare_img(style_img_path, np.asarray(content_img.shape[2:]), device)\n",
        "        init_img = style_img_resized\n",
        "\n",
        "    optimizing_img = Variable(init_img, requires_grad=True)\n",
        "\n",
        "    neural_net, content_feature_maps_index_name, style_feature_maps_indices_names = prepare_model(config['model'], device)\n",
        "    print(f'Using {config[\"model\"]} in the optimization procedure.')\n",
        "\n",
        "    content_img_set_of_feature_maps = neural_net(content_img)\n",
        "    style_img_set_of_feature_maps = neural_net(style_img)\n",
        "\n",
        "    target_content_representation = content_img_set_of_feature_maps[content_feature_maps_index_name[0]].squeeze(axis=0)\n",
        "    target_style_representation = [gram_matrix(x) for cnt, x in enumerate(style_img_set_of_feature_maps) if cnt in style_feature_maps_indices_names[0]]\n",
        "    target_representations = [target_content_representation, target_style_representation]\n",
        "\n",
        "    # for avoiding clutter\n",
        "    num_of_iterations = {\n",
        "        \"lbfgs\": 1000,\n",
        "        \"adam\": 3000,\n",
        "    }\n",
        "\n",
        "    # Once again the optimizing procedure begins\n",
        "    if config['optimizer'] == 'adam':\n",
        "        optimizer = Adam((optimizing_img,), lr=1e1)\n",
        "        tuning_step = make_tuning_step(neural_net, optimizer, target_representations, content_feature_maps_index_name[0], style_feature_maps_indices_names[0], config)\n",
        "        for cnt in range(num_of_iterations[config['optimizer']]):\n",
        "            total_loss, content_loss, style_loss, tv_loss = tuning_step(optimizing_img)\n",
        "            with torch.no_grad():\n",
        "                print(f'Adam | iteration: {cnt:03}, total loss={total_loss.item():12.4f}, content_loss={config[\"content_weight\"] * content_loss.item():12.4f}, style loss={config[\"style_weight\"] * style_loss.item():12.4f}, tv loss={config[\"tv_weight\"] * tv_loss.item():12.4f}')\n",
        "                save_and_maybe_display(optimizing_img, dump_path, config, cnt, num_of_iterations[config['optimizer']], should_display=False)\n",
        "    elif config['optimizer'] == 'lbfgs':\n",
        "\n",
        "        optimizer = LBFGS((optimizing_img,), max_iter=num_of_iterations['lbfgs'], line_search_fn='strong_wolfe')\n",
        "        cnt = 0\n",
        "\n",
        "        def closure():\n",
        "            nonlocal cnt\n",
        "            if torch.is_grad_enabled():\n",
        "                optimizer.zero_grad()\n",
        "            total_loss, content_loss, style_loss, tv_loss = build_loss(neural_net, optimizing_img, target_representations, content_feature_maps_index_name[0], style_feature_maps_indices_names[0], config)\n",
        "            if total_loss.requires_grad:\n",
        "                total_loss.backward()\n",
        "            with torch.no_grad():\n",
        "                print(f'L-BFGS | iteration: {cnt:03}, total loss={total_loss.item():12.4f}, content_loss={config[\"content_weight\"] * content_loss.item():12.4f}, style loss={config[\"style_weight\"] * style_loss.item():12.4f}, tv loss={config[\"tv_weight\"] * tv_loss.item():12.4f}')\n",
        "                save_and_maybe_display(optimizing_img, dump_path, config, cnt, num_of_iterations[config['optimizer']], should_display=False)\n",
        "\n",
        "            cnt += 1\n",
        "            return total_loss\n",
        "\n",
        "        optimizer.step(closure)\n",
        "\n",
        "    return dump_path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create necessary directories\n",
        "for folder in [\"data\", \"data/content-images\", \"data/style-images\", \"data/output-images\"]:\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "# Define paths\n",
        "default_resource_dir = os.path.abspath('data')\n",
        "content_images_dir = os.path.join(default_resource_dir, 'content-images')\n",
        "style_images_dir = os.path.join(default_resource_dir, 'style-images')\n",
        "output_img_dir = os.path.join(default_resource_dir, 'output-images')\n",
        "\n",
        "# Manually define parameters\n",
        "content_img_name = 'golden_gate.jpg'\n",
        "style_img_name = 'city.jpeg'\n",
        "height = 400\n",
        "\n",
        "content_weight = 1e5\n",
        "style_weight = 3e4\n",
        "tv_weight = 1e0\n",
        "\n",
        "optimizer = 'lbfgs'\n",
        "model = 'vgg19'\n",
        "init_method = 'content'\n",
        "saving_freq = -1\n",
        "\n",
        "# Configuration dictionary\n",
        "optimization_config = {\n",
        "    'content_images_dir': content_images_dir,\n",
        "    'style_images_dir': style_images_dir,\n",
        "    'output_img_dir': output_img_dir,\n",
        "    'content_img_name': content_img_name,\n",
        "    'style_img_name': style_img_name,\n",
        "    'height': height,\n",
        "    'content_weight': content_weight,\n",
        "    'style_weight': style_weight,\n",
        "    'tv_weight': tv_weight,\n",
        "    'optimizer': optimizer,\n",
        "    'model': model,\n",
        "    'init_method': init_method,\n",
        "    'saving_freq': saving_freq\n",
        "}\n",
        "\n",
        "# Running the nst func.\n",
        "if 'neural_style_transfer' in globals():\n",
        "    results_path = neural_style_transfer(optimization_config)\n",
        "else:\n",
        "    raise NotImplementedError(\"Function 'neural_style_transfer' is not defined.\")\n"
      ],
      "metadata": {
        "id": "xpwLxRlU3dpP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c7592270-b228-4930-f1f6-f8574e66f3c7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:04<00:00, 133MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using vgg19 in the optimization procedure.\n",
            "L-BFGS | iteration: 000, total loss=722859851776.0000, content_loss=      0.0000, style loss=722850060000.0000, tv loss=9782056.0000\n",
            "L-BFGS | iteration: 001, total loss=722859196416.0000, content_loss=      0.0015, style loss=722849460000.0000, tv loss=9782056.0000\n",
            "L-BFGS | iteration: 002, total loss=722855854080.0000, content_loss=      0.0369, style loss=722846100000.0000, tv loss=9782062.0000\n",
            "L-BFGS | iteration: 003, total loss=722850938880.0000, content_loss=      0.1860, style loss=722841180000.0000, tv loss=9782069.0000\n",
            "L-BFGS | iteration: 004, total loss=722773934080.0000, content_loss=     17.4801, style loss=722764200000.0000, tv loss=9782178.0000\n",
            "L-BFGS | iteration: 005, total loss=722387402752.0000, content_loss=    529.2674, style loss=722377620000.0000, tv loss=9782723.0000\n",
            "L-BFGS | iteration: 006, total loss=718164656128.0000, content_loss=  52526.4382, style loss=718154820000.0000, tv loss=9788726.0000\n",
            "L-BFGS | iteration: 007, total loss=678774177792.0000, content_loss=4806373.9777, style loss=678759540000.0000, tv loss=9848908.0000\n",
            "L-BFGS | iteration: 008, total loss=386824011776.0000, content_loss=280904003.9062, style loss=386532090000.0000, tv loss=11003287.0000\n",
            "L-BFGS | iteration: 009, total loss=234106372096.0000, content_loss=470957666.0156, style loss=233623125000.0000, tv loss=12287878.0000\n",
            "L-BFGS | iteration: 010, total loss=152670273536.0000, content_loss=632405078.1250, style loss=152024565000.0000, tv loss=13297019.0000\n",
            "L-BFGS | iteration: 011, total loss=117678194688.0000, content_loss=721916894.5312, style loss=116942355000.0000, tv loss=13920123.0000\n",
            "L-BFGS | iteration: 012, total loss=102227009536.0000, content_loss=803347753.9062, style loss=101409202500.0000, tv loss=14457514.0000\n",
            "L-BFGS | iteration: 013, total loss=87708565504.0000, content_loss=817171386.7188, style loss=86876737500.0000, tv loss=14666048.0000\n",
            "L-BFGS | iteration: 014, total loss=73860620288.0000, content_loss=858847558.5938, style loss=72986700000.0000, tv loss=15070778.0000\n",
            "L-BFGS | iteration: 015, total loss=61593919488.0000, content_loss=911807031.2500, style loss=60666431250.0000, tv loss=15683744.0000\n",
            "L-BFGS | iteration: 016, total loss=54193729536.0000, content_loss=928037890.6250, style loss=53249527500.0000, tv loss=16163764.0000\n",
            "L-BFGS | iteration: 017, total loss=48299540480.0000, content_loss=963052734.3750, style loss=47319922500.0000, tv loss=16566500.0000\n",
            "L-BFGS | iteration: 018, total loss=45282504704.0000, content_loss=961703417.9688, style loss=44304303750.0000, tv loss=16498064.0000\n",
            "L-BFGS | iteration: 019, total loss=40435245056.0000, content_loss=962656640.6250, style loss=39455921250.0000, tv loss=16665558.0000\n",
            "L-BFGS | iteration: 020, total loss=36383019008.0000, content_loss=992155371.0938, style loss=35374016250.0000, tv loss=16850660.0000\n",
            "L-BFGS | iteration: 021, total loss=33593602048.0000, content_loss=983285742.1875, style loss=32593121250.0000, tv loss=17194128.0000\n",
            "L-BFGS | iteration: 022, total loss=31107289088.0000, content_loss=996301367.1875, style loss=30093868125.0000, tv loss=17120020.0000\n",
            "L-BFGS | iteration: 023, total loss=28796983296.0000, content_loss=1020391015.6250, style loss=27759133125.0000, tv loss=17459712.0000\n",
            "L-BFGS | iteration: 024, total loss=26555932672.0000, content_loss=1034192089.8438, style loss=25504200000.0000, tv loss=17539826.0000\n",
            "L-BFGS | iteration: 025, total loss=25161488384.0000, content_loss=1020501562.5000, style loss=24123511875.0000, tv loss=17474030.0000\n",
            "L-BFGS | iteration: 026, total loss=23648012288.0000, content_loss=1040963671.8750, style loss=22589396250.0000, tv loss=17650916.0000\n",
            "L-BFGS | iteration: 027, total loss=22704855040.0000, content_loss=1053514648.4375, style loss=21633628125.0000, tv loss=17713120.0000\n",
            "L-BFGS | iteration: 028, total loss=21630849024.0000, content_loss=1059558398.4375, style loss=20553328125.0000, tv loss=17961452.0000\n",
            "L-BFGS | iteration: 029, total loss=20083599360.0000, content_loss=1060651171.8750, style loss=19004895000.0000, tv loss=18052924.0000\n",
            "L-BFGS | iteration: 030, total loss=18934749184.0000, content_loss=1068720019.5312, style loss=17847781875.0000, tv loss=18246848.0000\n",
            "L-BFGS | iteration: 031, total loss=18049968128.0000, content_loss=1063129980.4688, style loss=16968620625.0000, tv loss=18217716.0000\n",
            "L-BFGS | iteration: 032, total loss=17180827648.0000, content_loss=1062071191.4062, style loss=16100523750.0000, tv loss=18231480.0000\n",
            "L-BFGS | iteration: 033, total loss=16517297152.0000, content_loss=1064577343.7500, style loss=15434281875.0000, tv loss=18437920.0000\n",
            "L-BFGS | iteration: 034, total loss=15586078720.0000, content_loss=1073716699.2188, style loss=14493884062.5000, tv loss=18478152.0000\n",
            "L-BFGS | iteration: 035, total loss=15043645440.0000, content_loss=1067381933.5938, style loss=13957739062.5000, tv loss=18524336.0000\n",
            "L-BFGS | iteration: 036, total loss=14383365120.0000, content_loss=1075954296.8750, style loss=13288960312.5000, tv loss=18450888.0000\n",
            "L-BFGS | iteration: 037, total loss=13701305344.0000, content_loss=1087543750.0000, style loss=12595071562.5000, tv loss=18688774.0000\n",
            "L-BFGS | iteration: 038, total loss=13109029888.0000, content_loss=1086118066.4062, style loss=12004304062.5000, tv loss=18608508.0000\n",
            "L-BFGS | iteration: 039, total loss=12522834944.0000, content_loss=1087291308.5938, style loss=11416878750.0000, tv loss=18664574.0000\n",
            "L-BFGS | iteration: 040, total loss=11758064640.0000, content_loss=1091083691.4062, style loss=10648137187.5000, tv loss=18844524.0000\n",
            "L-BFGS | iteration: 041, total loss=11202860032.0000, content_loss=1089628808.5938, style loss=10094374687.5000, tv loss=18855548.0000\n",
            "L-BFGS | iteration: 042, total loss=10740684800.0000, content_loss=1091651757.8125, style loss=9630130312.5000, tv loss=18903460.0000\n",
            "L-BFGS | iteration: 043, total loss=10766111744.0000, content_loss=1083743750.0000, style loss=9663197812.5000, tv loss=19170676.0000\n",
            "L-BFGS | iteration: 044, total loss=10479265792.0000, content_loss=1088166699.2188, style loss=9372068437.5000, tv loss=19029580.0000\n",
            "L-BFGS | iteration: 045, total loss=10008833024.0000, content_loss=1091796972.6562, style loss=8897927812.5000, tv loss=19107982.0000\n",
            "L-BFGS | iteration: 046, total loss=9703147520.0000, content_loss=1092113964.8438, style loss=8591845312.5000, tv loss=19187318.0000\n",
            "L-BFGS | iteration: 047, total loss=9385946112.0000, content_loss=1096650195.3125, style loss=8270069062.5000, tv loss=19226752.0000\n",
            "L-BFGS | iteration: 048, total loss=9138833408.0000, content_loss=1095955859.3750, style loss=8023395000.0000, tv loss=19482472.0000\n",
            "L-BFGS | iteration: 049, total loss=8683666432.0000, content_loss=1100385742.1875, style loss=7563935156.2500, tv loss=19345512.0000\n",
            "L-BFGS | iteration: 050, total loss=8465480192.0000, content_loss=1097313476.5625, style loss=7348809843.7500, tv loss=19357428.0000\n",
            "L-BFGS | iteration: 051, total loss=8267815424.0000, content_loss=1097997949.2188, style loss=7150383750.0000, tv loss=19433734.0000\n",
            "L-BFGS | iteration: 052, total loss=8006524928.0000, content_loss=1100244531.2500, style loss=6886814531.2500, tv loss=19465970.0000\n",
            "L-BFGS | iteration: 053, total loss=7778673664.0000, content_loss=1101540332.0312, style loss=6657466406.2500, tv loss=19666800.0000\n",
            "L-BFGS | iteration: 054, total loss=7509101056.0000, content_loss=1104030957.0312, style loss=6385575000.0000, tv loss=19495550.0000\n",
            "L-BFGS | iteration: 055, total loss=7387152384.0000, content_loss=1103817187.5000, style loss=6263772656.2500, tv loss=19562404.0000\n",
            "L-BFGS | iteration: 056, total loss=7220109824.0000, content_loss=1107743359.3750, style loss=6092668593.7500, tv loss=19698222.0000\n",
            "L-BFGS | iteration: 057, total loss=6998822912.0000, content_loss=1105820117.1875, style loss=5873213906.2500, tv loss=19788908.0000\n",
            "L-BFGS | iteration: 058, total loss=6741274624.0000, content_loss=1111275390.6250, style loss=5610139218.7500, tv loss=19860092.0000\n",
            "L-BFGS | iteration: 059, total loss=6586400256.0000, content_loss=1101566992.1875, style loss=5464955156.2500, tv loss=19877788.0000\n",
            "L-BFGS | iteration: 060, total loss=6464108544.0000, content_loss=1101297070.3125, style loss=5342995312.5000, tv loss=19815742.0000\n",
            "L-BFGS | iteration: 061, total loss=6323926528.0000, content_loss=1103274902.3438, style loss=5200744687.5000, tv loss=19907294.0000\n",
            "L-BFGS | iteration: 062, total loss=6179826688.0000, content_loss=1105488378.9062, style loss=5054378906.2500, tv loss=19959202.0000\n",
            "L-BFGS | iteration: 063, total loss=5964079616.0000, content_loss=1105176757.8125, style loss=4838860781.2500, tv loss=20042290.0000\n",
            "L-BFGS | iteration: 064, total loss=5824636416.0000, content_loss=1103627636.7188, style loss=4700998125.0000, tv loss=20010280.0000\n",
            "L-BFGS | iteration: 065, total loss=5737891840.0000, content_loss=1104913476.5625, style loss=4612968281.2500, tv loss=20009972.0000\n",
            "L-BFGS | iteration: 066, total loss=5673487360.0000, content_loss=1098600488.2812, style loss=4554757031.2500, tv loss=20129788.0000\n",
            "L-BFGS | iteration: 067, total loss=5551090176.0000, content_loss=1101443847.6562, style loss=4429540781.2500, tv loss=20105508.0000\n",
            "L-BFGS | iteration: 068, total loss=5445550080.0000, content_loss=1100923925.7812, style loss=4324501875.0000, tv loss=20124336.0000\n",
            "L-BFGS | iteration: 069, total loss=5314650112.0000, content_loss=1098164746.0938, style loss=4196296875.0000, tv loss=20188132.0000\n",
            "L-BFGS | iteration: 070, total loss=5185529344.0000, content_loss=1094609960.9375, style loss=4070632968.7500, tv loss=20286718.0000\n",
            "L-BFGS | iteration: 071, total loss=5062401536.0000, content_loss=1097313183.5938, style loss=3944802187.5000, tv loss=20285792.0000\n",
            "L-BFGS | iteration: 072, total loss=5005346304.0000, content_loss=1095170019.5312, style loss=3889845468.7500, tv loss=20331078.0000\n",
            "L-BFGS | iteration: 073, total loss=4934600192.0000, content_loss=1095743359.3750, style loss=3818553750.0000, tv loss=20302856.0000\n",
            "L-BFGS | iteration: 074, total loss=4863504896.0000, content_loss=1095352539.0625, style loss=3747825000.0000, tv loss=20327374.0000\n",
            "L-BFGS | iteration: 075, total loss=4781196800.0000, content_loss=1095589550.7812, style loss=3665276015.6250, tv loss=20331452.0000\n",
            "L-BFGS | iteration: 076, total loss=4686033408.0000, content_loss=1094052832.0312, style loss=3571567500.0000, tv loss=20412818.0000\n",
            "L-BFGS | iteration: 077, total loss=4589875200.0000, content_loss=1094001562.5000, style loss=3475461796.8750, tv loss=20411656.0000\n",
            "L-BFGS | iteration: 078, total loss=4536297984.0000, content_loss=1093194335.9375, style loss=3422652890.6250, tv loss=20450926.0000\n",
            "L-BFGS | iteration: 079, total loss=4510260736.0000, content_loss=1087350878.9062, style loss=3402360703.1250, tv loss=20549200.0000\n",
            "L-BFGS | iteration: 080, total loss=4426358784.0000, content_loss=1089851855.4688, style loss=3315973125.0000, tv loss=20533744.0000\n",
            "L-BFGS | iteration: 081, total loss=4382373888.0000, content_loss=1089709570.3125, style loss=3272106093.7500, tv loss=20558164.0000\n",
            "L-BFGS | iteration: 082, total loss=4330927616.0000, content_loss=1081333691.4062, style loss=3229002421.8750, tv loss=20591496.0000\n",
            "L-BFGS | iteration: 083, total loss=4252900608.0000, content_loss=1087681250.0000, style loss=3144657656.2500, tv loss=20561718.0000\n",
            "L-BFGS | iteration: 084, total loss=4229863680.0000, content_loss=1087482421.8750, style loss=3121796953.1250, tv loss=20584476.0000\n",
            "L-BFGS | iteration: 085, total loss=4197128192.0000, content_loss=1081392480.4688, style loss=3095031796.8750, tv loss=20703844.0000\n",
            "L-BFGS | iteration: 086, total loss=4163333632.0000, content_loss=1083863281.2500, style loss=3058816406.2500, tv loss=20653932.0000\n",
            "L-BFGS | iteration: 087, total loss=4113189888.0000, content_loss=1084121484.3750, style loss=3008405390.6250, tv loss=20662660.0000\n",
            "L-BFGS | iteration: 088, total loss=4050920704.0000, content_loss=1082583300.7812, style loss=2947650468.7500, tv loss=20686728.0000\n",
            "L-BFGS | iteration: 089, total loss=3984949760.0000, content_loss=1080446289.0625, style loss=2883795703.1250, tv loss=20707800.0000\n",
            "L-BFGS | iteration: 090, total loss=3924095744.0000, content_loss=1079066894.5312, style loss=2824283671.8750, tv loss=20745164.0000\n",
            "L-BFGS | iteration: 091, total loss=3897299456.0000, content_loss=1080416894.5312, style loss=2796149296.8750, tv loss=20733380.0000\n",
            "L-BFGS | iteration: 092, total loss=3861174528.0000, content_loss=1076280273.4375, style loss=2764062421.8750, tv loss=20831824.0000\n",
            "L-BFGS | iteration: 093, total loss=3817696256.0000, content_loss=1077934179.6875, style loss=2718961171.8750, tv loss=20800928.0000\n",
            "L-BFGS | iteration: 094, total loss=3778590720.0000, content_loss=1078982812.5000, style loss=2678779453.1250, tv loss=20828570.0000\n",
            "L-BFGS | iteration: 095, total loss=3738895616.0000, content_loss=1077219433.5938, style loss=2640801093.7500, tv loss=20874996.0000\n",
            "L-BFGS | iteration: 096, total loss=3705753600.0000, content_loss=1078051660.1562, style loss=2606805468.7500, tv loss=20896372.0000\n",
            "L-BFGS | iteration: 097, total loss=3680837632.0000, content_loss=1077163964.8438, style loss=2582761640.6250, tv loss=20912172.0000\n",
            "L-BFGS | iteration: 098, total loss=3625388800.0000, content_loss=1073434960.9375, style loss=2530969921.8750, tv loss=20983980.0000\n",
            "L-BFGS | iteration: 099, total loss=3589078528.0000, content_loss=1075110058.5938, style loss=2493032578.1250, tv loss=20935718.0000\n",
            "L-BFGS | iteration: 100, total loss=3560694016.0000, content_loss=1073559082.0312, style loss=2466163828.1250, tv loss=20971354.0000\n",
            "L-BFGS | iteration: 101, total loss=3531657216.0000, content_loss=1070714843.7500, style loss=2439915703.1250, tv loss=21026580.0000\n",
            "L-BFGS | iteration: 102, total loss=3505558016.0000, content_loss=1071230273.4375, style loss=2413309687.5000, tv loss=21018098.0000\n",
            "L-BFGS | iteration: 103, total loss=3461284608.0000, content_loss=1070665332.0312, style loss=2369579765.6250, tv loss=21039316.0000\n",
            "L-BFGS | iteration: 104, total loss=3414542336.0000, content_loss=1068457421.8750, style loss=2325023437.5000, tv loss=21061252.0000\n",
            "L-BFGS | iteration: 105, total loss=3375419136.0000, content_loss=1066843164.0625, style loss=2287447968.7500, tv loss=21127964.0000\n",
            "L-BFGS | iteration: 106, total loss=3349560064.0000, content_loss=1068614355.4688, style loss=2259826640.6250, tv loss=21118998.0000\n",
            "L-BFGS | iteration: 107, total loss=3323270400.0000, content_loss=1065757617.1875, style loss=2236356328.1250, tv loss=21156392.0000\n",
            "L-BFGS | iteration: 108, total loss=3302075392.0000, content_loss=1065502539.0625, style loss=2215401328.1250, tv loss=21171760.0000\n",
            "L-BFGS | iteration: 109, total loss=3258797056.0000, content_loss=1064674511.7188, style loss=2172919453.1250, tv loss=21203050.0000\n",
            "L-BFGS | iteration: 110, total loss=3242087936.0000, content_loss=1066169726.5625, style loss=2154657890.6250, tv loss=21260384.0000\n",
            "L-BFGS | iteration: 111, total loss=3215957248.0000, content_loss=1066216113.2812, style loss=2128511953.1250, tv loss=21229000.0000\n",
            "L-BFGS | iteration: 112, total loss=3199586816.0000, content_loss=1064639062.5000, style loss=2113692421.8750, tv loss=21255312.0000\n",
            "L-BFGS | iteration: 113, total loss=3180137984.0000, content_loss=1065517187.5000, style loss=2093377265.6250, tv loss=21243276.0000\n",
            "L-BFGS | iteration: 114, total loss=3154073344.0000, content_loss=1065274023.4375, style loss=2067537421.8750, tv loss=21261810.0000\n",
            "L-BFGS | iteration: 115, total loss=3116300544.0000, content_loss=1064044140.6250, style loss=2030936953.1250, tv loss=21319356.0000\n",
            "L-BFGS | iteration: 116, total loss=3079390720.0000, content_loss=1061722656.2500, style loss=1996342734.3750, tv loss=21325200.0000\n",
            "L-BFGS | iteration: 117, total loss=3053695744.0000, content_loss=1059345800.7812, style loss=1972989140.6250, tv loss=21360860.0000\n",
            "L-BFGS | iteration: 118, total loss=3032265472.0000, content_loss=1059245800.7812, style loss=1951663710.9375, tv loss=21355862.0000\n",
            "L-BFGS | iteration: 119, total loss=3009377536.0000, content_loss=1055488476.5625, style loss=1932500273.4375, tv loss=21388796.0000\n",
            "L-BFGS | iteration: 120, total loss=2989146624.0000, content_loss=1054871484.3750, style loss=1912877460.9375, tv loss=21397692.0000\n",
            "L-BFGS | iteration: 121, total loss=2955789056.0000, content_loss=1053759667.9688, style loss=1880583984.3750, tv loss=21445464.0000\n",
            "L-BFGS | iteration: 122, total loss=2924169216.0000, content_loss=1052691601.5625, style loss=1850028632.8125, tv loss=21448880.0000\n",
            "L-BFGS | iteration: 123, total loss=2895276800.0000, content_loss=1051736523.4375, style loss=1822039335.9375, tv loss=21500978.0000\n",
            "L-BFGS | iteration: 124, total loss=2877939968.0000, content_loss=1053682226.5625, style loss=1802754726.5625, tv loss=21502992.0000\n",
            "L-BFGS | iteration: 125, total loss=2868189952.0000, content_loss=1049923242.1875, style loss=1796698476.5625, tv loss=21568228.0000\n",
            "L-BFGS | iteration: 126, total loss=2858811904.0000, content_loss=1051401464.8438, style loss=1785868710.9375, tv loss=21541912.0000\n",
            "L-BFGS | iteration: 127, total loss=2842997248.0000, content_loss=1050127929.6875, style loss=1771331250.0000, tv loss=21538044.0000\n",
            "L-BFGS | iteration: 128, total loss=2815527424.0000, content_loss=1049048242.1875, style loss=1744925507.8125, tv loss=21553550.0000\n",
            "L-BFGS | iteration: 129, total loss=2792623872.0000, content_loss=1047487402.3438, style loss=1723535273.4375, tv loss=21601192.0000\n",
            "L-BFGS | iteration: 130, total loss=2769318656.0000, content_loss=1049210644.5312, style loss=1698510820.3125, tv loss=21597088.0000\n",
            "L-BFGS | iteration: 131, total loss=2760425728.0000, content_loss=1049383007.8125, style loss=1689383085.9375, tv loss=21659620.0000\n",
            "L-BFGS | iteration: 132, total loss=2739535104.0000, content_loss=1051667382.8125, style loss=1666218632.8125, tv loss=21649144.0000\n",
            "L-BFGS | iteration: 133, total loss=2726665728.0000, content_loss=1050517871.0938, style loss=1654509960.9375, tv loss=21637982.0000\n",
            "L-BFGS | iteration: 134, total loss=2714843136.0000, content_loss=1049452050.7812, style loss=1643741250.0000, tv loss=21650008.0000\n",
            "L-BFGS | iteration: 135, total loss=2698167040.0000, content_loss=1047867773.4375, style loss=1628631562.5000, tv loss=21667684.0000\n",
            "L-BFGS | iteration: 136, total loss=2667180288.0000, content_loss=1045091894.5312, style loss=1600378476.5625, tv loss=21709700.0000\n",
            "L-BFGS | iteration: 137, total loss=2637360640.0000, content_loss=1042531054.6875, style loss=1573079414.0625, tv loss=21750156.0000\n",
            "L-BFGS | iteration: 138, total loss=2621367296.0000, content_loss=1038685253.9062, style loss=1560870000.0000, tv loss=21812006.0000\n",
            "L-BFGS | iteration: 139, total loss=2609058816.0000, content_loss=1039745214.8438, style loss=1547543671.8750, tv loss=21770028.0000\n",
            "L-BFGS | iteration: 140, total loss=2589760768.0000, content_loss=1040192285.1562, style loss=1527774257.8125, tv loss=21794214.0000\n",
            "L-BFGS | iteration: 141, total loss=2580167168.0000, content_loss=1039549902.3438, style loss=1518797812.5000, tv loss=21819488.0000\n",
            "L-BFGS | iteration: 142, total loss=2566734080.0000, content_loss=1039351562.5000, style loss=1505543906.2500, tv loss=21838552.0000\n",
            "L-BFGS | iteration: 143, total loss=2541817600.0000, content_loss=1038733203.1250, style loss=1481224335.9375, tv loss=21860008.0000\n",
            "L-BFGS | iteration: 144, total loss=2527053056.0000, content_loss=1038823730.4688, style loss=1466346210.9375, tv loss=21883104.0000\n",
            "L-BFGS | iteration: 145, total loss=2510355712.0000, content_loss=1037444335.9375, style loss=1451033671.8750, tv loss=21877612.0000\n",
            "L-BFGS | iteration: 146, total loss=2500140800.0000, content_loss=1036068457.0312, style loss=1442186250.0000, tv loss=21886200.0000\n",
            "L-BFGS | iteration: 147, total loss=2490396928.0000, content_loss=1035740136.7188, style loss=1432765195.3125, tv loss=21891688.0000\n",
            "L-BFGS | iteration: 148, total loss=2482889472.0000, content_loss=1033046093.7500, style loss=1427878007.8125, tv loss=21965334.0000\n",
            "L-BFGS | iteration: 149, total loss=2458118144.0000, content_loss=1034733593.7500, style loss=1401433359.3750, tv loss=21951260.0000\n",
            "L-BFGS | iteration: 150, total loss=2447634944.0000, content_loss=1034848046.8750, style loss=1390822851.5625, tv loss=21964012.0000\n",
            "L-BFGS | iteration: 151, total loss=2427153408.0000, content_loss=1033649707.0312, style loss=1371463828.1250, tv loss=22039902.0000\n",
            "L-BFGS | iteration: 152, total loss=2409367552.0000, content_loss=1035007031.2500, style loss=1352347031.2500, tv loss=22013512.0000\n",
            "L-BFGS | iteration: 153, total loss=2397605632.0000, content_loss=1033625781.2500, style loss=1341953320.3125, tv loss=22026560.0000\n",
            "L-BFGS | iteration: 154, total loss=2384224256.0000, content_loss=1031115039.0625, style loss=1331030039.0625, tv loss=22079228.0000\n",
            "L-BFGS | iteration: 155, total loss=2371685632.0000, content_loss=1030532226.5625, style loss=1319064257.8125, tv loss=22089052.0000\n",
            "L-BFGS | iteration: 156, total loss=2355558656.0000, content_loss=1028999023.4375, style loss=1304446640.6250, tv loss=22112902.0000\n",
            "L-BFGS | iteration: 157, total loss=2339386112.0000, content_loss=1028699902.3438, style loss=1288558359.3750, tv loss=22127948.0000\n",
            "L-BFGS | iteration: 158, total loss=2326567936.0000, content_loss=1026872265.6250, style loss=1277547656.2500, tv loss=22148158.0000\n",
            "L-BFGS | iteration: 159, total loss=2314043648.0000, content_loss=1027073925.7812, style loss=1264837734.3750, tv loss=22131968.0000\n",
            "L-BFGS | iteration: 160, total loss=2305287680.0000, content_loss=1025653808.5938, style loss=1257450351.5625, tv loss=22183332.0000\n",
            "L-BFGS | iteration: 161, total loss=2293685504.0000, content_loss=1025667285.1562, style loss=1245844570.3125, tv loss=22173626.0000\n",
            "L-BFGS | iteration: 162, total loss=2281726976.0000, content_loss=1024388574.2188, style loss=1235155195.3125, tv loss=22183198.0000\n",
            "L-BFGS | iteration: 163, total loss=2271064576.0000, content_loss=1024452246.0938, style loss=1224413671.8750, tv loss=22198868.0000\n",
            "L-BFGS | iteration: 164, total loss=2259580928.0000, content_loss=1022791308.5938, style loss=1214539335.9375, tv loss=22250236.0000\n",
            "L-BFGS | iteration: 165, total loss=2244862976.0000, content_loss=1024116503.9062, style loss=1198499414.0625, tv loss=22246824.0000\n",
            "L-BFGS | iteration: 166, total loss=2237914112.0000, content_loss=1024050097.6562, style loss=1191609257.8125, tv loss=22254790.0000\n",
            "L-BFGS | iteration: 167, total loss=2220613888.0000, content_loss=1022695312.5000, style loss=1175637539.0625, tv loss=22281084.0000\n",
            "L-BFGS | iteration: 168, total loss=2208058880.0000, content_loss=1021774511.7188, style loss=1163985234.3750, tv loss=22299148.0000\n",
            "L-BFGS | iteration: 169, total loss=2197478912.0000, content_loss=1020923144.5312, style loss=1154237578.1250, tv loss=22318072.0000\n",
            "L-BFGS | iteration: 170, total loss=2183802880.0000, content_loss=1019071386.7188, style loss=1142360039.0625, tv loss=22371274.0000\n",
            "L-BFGS | iteration: 171, total loss=2183161344.0000, content_loss=1019932128.9062, style loss=1140858164.0625, tv loss=22371092.0000\n",
            "L-BFGS | iteration: 172, total loss=2165987328.0000, content_loss=1018201562.5000, style loss=1125378164.0625, tv loss=22407736.0000\n",
            "L-BFGS | iteration: 173, total loss=2157740032.0000, content_loss=1017893261.7188, style loss=1117436250.0000, tv loss=22410468.0000\n",
            "L-BFGS | iteration: 174, total loss=2147031040.0000, content_loss=1017114648.4375, style loss=1107496289.0625, tv loss=22420108.0000\n",
            "L-BFGS | iteration: 175, total loss=2132269824.0000, content_loss=1015974511.7188, style loss=1093854492.1875, tv loss=22440786.0000\n",
            "L-BFGS | iteration: 176, total loss=2123692416.0000, content_loss=1014136816.4062, style loss=1087095351.5625, tv loss=22460252.0000\n",
            "L-BFGS | iteration: 177, total loss=2118096384.0000, content_loss=1015309960.9375, style loss=1080355664.0625, tv loss=22430692.0000\n",
            "L-BFGS | iteration: 178, total loss=2108805248.0000, content_loss=1014810839.8438, style loss=1071528984.3750, tv loss=22465450.0000\n",
            "L-BFGS | iteration: 179, total loss=2103542784.0000, content_loss=1013759667.9688, style loss=1067294648.4375, tv loss=22488426.0000\n",
            "L-BFGS | iteration: 180, total loss=2094718848.0000, content_loss=1013009179.6875, style loss=1059204140.6250, tv loss=22505644.0000\n",
            "L-BFGS | iteration: 181, total loss=2077418496.0000, content_loss=1011400878.9062, style loss=1043473710.9375, tv loss=22543888.0000\n",
            "L-BFGS | iteration: 182, total loss=2071193344.0000, content_loss=1010960253.9062, style loss=1037686757.8125, tv loss=22546244.0000\n",
            "L-BFGS | iteration: 183, total loss=2061149440.0000, content_loss=1010486621.0938, style loss=1028103398.4375, tv loss=22559444.0000\n",
            "L-BFGS | iteration: 184, total loss=2055633920.0000, content_loss=1010282910.1562, style loss=1022789414.0625, tv loss=22561504.0000\n",
            "L-BFGS | iteration: 185, total loss=2050002176.0000, content_loss=1008932714.8438, style loss=1018481601.5625, tv loss=22587856.0000\n",
            "L-BFGS | iteration: 186, total loss=2041041920.0000, content_loss=1008750781.2500, style loss=1009699101.5625, tv loss=22592044.0000\n",
            "L-BFGS | iteration: 187, total loss=2031732224.0000, content_loss=1008106152.3438, style loss=1001017148.4375, tv loss=22608848.0000\n",
            "L-BFGS | iteration: 188, total loss=2020288000.0000, content_loss=1007115625.0000, style loss=990533437.5000, tv loss=22638888.0000\n",
            "L-BFGS | iteration: 189, total loss=2011147776.0000, content_loss=1006716992.1875, style loss=981775312.5000, tv loss=22655452.0000\n",
            "L-BFGS | iteration: 190, total loss=2002734848.0000, content_loss=1006234960.9375, style loss=973826718.7500, tv loss=22673164.0000\n",
            "L-BFGS | iteration: 191, total loss=1996891008.0000, content_loss=1006020703.1250, style loss=968202949.2188, tv loss=22667350.0000\n",
            "L-BFGS | iteration: 192, total loss=1995971584.0000, content_loss=1003589843.7500, style loss=969676992.1875, tv loss=22704770.0000\n",
            "L-BFGS | iteration: 193, total loss=1992790144.0000, content_loss=1004652050.7812, style loss=965450449.2188, tv loss=22687584.0000\n",
            "L-BFGS | iteration: 194, total loss=1986880256.0000, content_loss=1004167089.8438, style loss=960022324.2188, tv loss=22690756.0000\n",
            "L-BFGS | iteration: 195, total loss=1977630336.0000, content_loss=1002706933.5938, style loss=952226074.2188, tv loss=22697358.0000\n",
            "L-BFGS | iteration: 196, total loss=1970031616.0000, content_loss=1002223730.4688, style loss=945081855.4688, tv loss=22726048.0000\n",
            "L-BFGS | iteration: 197, total loss=1962594176.0000, content_loss=1001463378.9062, style loss=938392792.9688, tv loss=22738028.0000\n",
            "L-BFGS | iteration: 198, total loss=1953821952.0000, content_loss=1000689257.8125, style loss=930372949.2188, tv loss=22759624.0000\n",
            "L-BFGS | iteration: 199, total loss=1945740928.0000, content_loss=1001152050.7812, style loss=921812753.9062, tv loss=22776216.0000\n",
            "L-BFGS | iteration: 200, total loss=1939868800.0000, content_loss=999323437.5000, style loss=917743886.7188, tv loss=22801580.0000\n",
            "L-BFGS | iteration: 201, total loss=1935391488.0000, content_loss=998862988.2812, style loss=913750429.6875, tv loss=22778120.0000\n",
            "L-BFGS | iteration: 202, total loss=1928705280.0000, content_loss=998854101.5625, style loss=907057031.2500, tv loss=22794188.0000\n",
            "L-BFGS | iteration: 203, total loss=1923904000.0000, content_loss=998491503.9062, style loss=902601035.1562, tv loss=22811340.0000\n",
            "L-BFGS | iteration: 204, total loss=1918707968.0000, content_loss=998320214.8438, style loss=897561855.4688, tv loss=22825976.0000\n",
            "L-BFGS | iteration: 205, total loss=1908611840.0000, content_loss=997994531.2500, style loss=887767792.9688, tv loss=22849488.0000\n",
            "L-BFGS | iteration: 206, total loss=1898311808.0000, content_loss=997769238.2812, style loss=877665175.7812, tv loss=22877432.0000\n",
            "L-BFGS | iteration: 207, total loss=1891089920.0000, content_loss=996441308.5938, style loss=871759160.1562, tv loss=22889438.0000\n",
            "L-BFGS | iteration: 208, total loss=1887645184.0000, content_loss=996389355.4688, style loss=868382988.2812, tv loss=22872820.0000\n",
            "L-BFGS | iteration: 209, total loss=1881538688.0000, content_loss=995150683.5938, style loss=863483437.5000, tv loss=22904624.0000\n",
            "L-BFGS | iteration: 210, total loss=1876795648.0000, content_loss=994467382.8125, style loss=859412929.6875, tv loss=22915306.0000\n",
            "L-BFGS | iteration: 211, total loss=1869137664.0000, content_loss=993010644.5312, style loss=853195019.5312, tv loss=22931996.0000\n",
            "L-BFGS | iteration: 212, total loss=1862708864.0000, content_loss=991013964.8438, style loss=848751855.4688, tv loss=22943104.0000\n",
            "L-BFGS | iteration: 213, total loss=1856062976.0000, content_loss=990071191.4062, style loss=843044062.5000, tv loss=22947800.0000\n",
            "L-BFGS | iteration: 214, total loss=1850948480.0000, content_loss=989262500.0000, style loss=838713750.0000, tv loss=22972272.0000\n",
            "L-BFGS | iteration: 215, total loss=1844030336.0000, content_loss=989513476.5625, style loss=831544042.9688, tv loss=22972776.0000\n",
            "L-BFGS | iteration: 216, total loss=1840315008.0000, content_loss=988496875.0000, style loss=828821191.4062, tv loss=22996880.0000\n",
            "L-BFGS | iteration: 217, total loss=1836500224.0000, content_loss=988920605.4688, style loss=824592597.6562, tv loss=22986964.0000\n",
            "L-BFGS | iteration: 218, total loss=1830513408.0000, content_loss=988359277.3438, style loss=819164238.2812, tv loss=22989772.0000\n",
            "L-BFGS | iteration: 219, total loss=1826812416.0000, content_loss=987985839.8438, style loss=815829960.9375, tv loss=22996628.0000\n",
            "L-BFGS | iteration: 220, total loss=1821197056.0000, content_loss=987365234.3750, style loss=810821191.4062, tv loss=23010538.0000\n",
            "L-BFGS | iteration: 221, total loss=1813948032.0000, content_loss=987018750.0000, style loss=803901562.5000, tv loss=23027676.0000\n",
            "L-BFGS | iteration: 222, total loss=1808521856.0000, content_loss=986605566.4062, style loss=798866191.4062, tv loss=23050124.0000\n",
            "L-BFGS | iteration: 223, total loss=1803878656.0000, content_loss=986774902.3438, style loss=794055937.5000, tv loss=23047792.0000\n",
            "L-BFGS | iteration: 224, total loss=1799905408.0000, content_loss=985572753.9062, style loss=791256679.6875, tv loss=23075960.0000\n",
            "L-BFGS | iteration: 225, total loss=1795791232.0000, content_loss=985586523.4375, style loss=787126992.1875, tv loss=23077796.0000\n",
            "L-BFGS | iteration: 226, total loss=1789652608.0000, content_loss=984571875.0000, style loss=781997050.7812, tv loss=23083664.0000\n",
            "L-BFGS | iteration: 227, total loss=1784469888.0000, content_loss=984028808.5938, style loss=777349335.9375, tv loss=23091830.0000\n",
            "L-BFGS | iteration: 228, total loss=1779457280.0000, content_loss=982704101.5625, style loss=773649492.1875, tv loss=23103714.0000\n",
            "L-BFGS | iteration: 229, total loss=1774589056.0000, content_loss=982203320.3125, style loss=769281035.1562, tv loss=23104580.0000\n",
            "L-BFGS | iteration: 230, total loss=1772077824.0000, content_loss=980677539.0625, style loss=768253183.5938, tv loss=23147004.0000\n",
            "L-BFGS | iteration: 231, total loss=1764975232.0000, content_loss=981207031.2500, style loss=760638984.3750, tv loss=23129166.0000\n",
            "L-BFGS | iteration: 232, total loss=1761839872.0000, content_loss=980803027.3438, style loss=757905703.1250, tv loss=23131094.0000\n",
            "L-BFGS | iteration: 233, total loss=1757299584.0000, content_loss=980012402.3438, style loss=754140527.3438, tv loss=23146652.0000\n",
            "L-BFGS | iteration: 234, total loss=1750614144.0000, content_loss=979478027.3438, style loss=747974941.4062, tv loss=23161260.0000\n",
            "L-BFGS | iteration: 235, total loss=1745848832.0000, content_loss=978037792.9688, style loss=744625605.4688, tv loss=23185414.0000\n",
            "L-BFGS | iteration: 236, total loss=1740895872.0000, content_loss=978454492.1875, style loss=739264921.8750, tv loss=23176536.0000\n",
            "L-BFGS | iteration: 237, total loss=1738229760.0000, content_loss=977634472.6562, style loss=737409667.9688, tv loss=23185640.0000\n",
            "L-BFGS | iteration: 238, total loss=1734941696.0000, content_loss=977082421.8750, style loss=734672402.3438, tv loss=23186996.0000\n",
            "L-BFGS | iteration: 239, total loss=1729077248.0000, content_loss=976015625.0000, style loss=729857050.7812, tv loss=23204672.0000\n",
            "L-BFGS | iteration: 240, total loss=1725764864.0000, content_loss=976057031.2500, style loss=726507187.5000, tv loss=23200582.0000\n",
            "L-BFGS | iteration: 241, total loss=1722444160.0000, content_loss=976036132.8125, style loss=723200566.4062, tv loss=23207372.0000\n",
            "L-BFGS | iteration: 242, total loss=1719398016.0000, content_loss=975962402.3438, style loss=720218847.6562, tv loss=23216762.0000\n",
            "L-BFGS | iteration: 243, total loss=1716894592.0000, content_loss=976153417.9688, style loss=717522656.2500, tv loss=23218564.0000\n",
            "L-BFGS | iteration: 244, total loss=1714387840.0000, content_loss=974997851.5625, style loss=716147050.7812, tv loss=23242916.0000\n",
            "L-BFGS | iteration: 245, total loss=1710229248.0000, content_loss=975154882.8125, style loss=711838593.7500, tv loss=23235826.0000\n",
            "L-BFGS | iteration: 246, total loss=1707134720.0000, content_loss=974637597.6562, style loss=709257187.5000, tv loss=23239896.0000\n",
            "L-BFGS | iteration: 247, total loss=1700488704.0000, content_loss=973075878.9062, style loss=704153203.1250, tv loss=23259608.0000\n",
            "L-BFGS | iteration: 248, total loss=1697271552.0000, content_loss=972514355.4688, style loss=701480800.7812, tv loss=23276344.0000\n",
            "L-BFGS | iteration: 249, total loss=1692796544.0000, content_loss=972214648.4375, style loss=697307402.3438, tv loss=23274492.0000\n",
            "L-BFGS | iteration: 250, total loss=1690318464.0000, content_loss=971652929.6875, style loss=695382187.5000, tv loss=23283302.0000\n",
            "L-BFGS | iteration: 251, total loss=1687758592.0000, content_loss=971840527.3438, style loss=692637246.0938, tv loss=23280896.0000\n",
            "L-BFGS | iteration: 252, total loss=1684245888.0000, content_loss=971237011.7188, style loss=689714062.5000, tv loss=23294900.0000\n",
            "L-BFGS | iteration: 253, total loss=1679850880.0000, content_loss=970190527.3438, style loss=686362558.5938, tv loss=23297796.0000\n",
            "L-BFGS | iteration: 254, total loss=1676681728.0000, content_loss=970192382.8125, style loss=683189472.6562, tv loss=23299832.0000\n",
            "L-BFGS | iteration: 255, total loss=1674466816.0000, content_loss=969402148.4375, style loss=681759667.9688, tv loss=23304984.0000\n",
            "L-BFGS | iteration: 256, total loss=1669988096.0000, content_loss=968241601.5625, style loss=678431250.0000, tv loss=23315176.0000\n",
            "L-BFGS | iteration: 257, total loss=1665774464.0000, content_loss=967554687.5000, style loss=674887968.7500, tv loss=23331772.0000\n",
            "L-BFGS | iteration: 258, total loss=1661757696.0000, content_loss=967475000.0000, style loss=670945722.6562, tv loss=23336932.0000\n",
            "L-BFGS | iteration: 259, total loss=1659278080.0000, content_loss=966912500.0000, style loss=669009609.3750, tv loss=23355956.0000\n",
            "L-BFGS | iteration: 260, total loss=1655844224.0000, content_loss=967047558.5938, style loss=665442597.6562, tv loss=23353980.0000\n",
            "L-BFGS | iteration: 261, total loss=1653244928.0000, content_loss=966889355.4688, style loss=662998417.9688, tv loss=23357214.0000\n",
            "L-BFGS | iteration: 262, total loss=1650032896.0000, content_loss=966211914.0625, style loss=660457792.9688, tv loss=23363210.0000\n",
            "L-BFGS | iteration: 263, total loss=1646312704.0000, content_loss=965733496.0938, style loss=657205019.5312, tv loss=23374252.0000\n",
            "L-BFGS | iteration: 264, total loss=1643011328.0000, content_loss=965021777.3438, style loss=654610195.3125, tv loss=23379400.0000\n",
            "L-BFGS | iteration: 265, total loss=1640577408.0000, content_loss=965014257.8125, style loss=652184941.4062, tv loss=23378184.0000\n",
            "L-BFGS | iteration: 266, total loss=1638688000.0000, content_loss=964346191.4062, style loss=650946093.7500, tv loss=23395666.0000\n",
            "L-BFGS | iteration: 267, total loss=1635663616.0000, content_loss=964560156.2500, style loss=647714179.6875, tv loss=23389360.0000\n",
            "L-BFGS | iteration: 268, total loss=1632366720.0000, content_loss=964081738.2812, style loss=644893535.1562, tv loss=23391504.0000\n",
            "L-BFGS | iteration: 269, total loss=1629250688.0000, content_loss=963271875.0000, style loss=642578496.0938, tv loss=23400312.0000\n",
            "L-BFGS | iteration: 270, total loss=1626775040.0000, content_loss=962621289.0625, style loss=640746035.1562, tv loss=23407604.0000\n",
            "L-BFGS | iteration: 271, total loss=1624187904.0000, content_loss=962055175.7812, style loss=638718339.8438, tv loss=23414386.0000\n",
            "L-BFGS | iteration: 272, total loss=1619614336.0000, content_loss=961132031.2500, style loss=635049550.7812, tv loss=23432776.0000\n",
            "L-BFGS | iteration: 273, total loss=1616688640.0000, content_loss=960961718.7500, style loss=632296171.8750, tv loss=23430672.0000\n",
            "L-BFGS | iteration: 274, total loss=1614468480.0000, content_loss=960435644.5312, style loss=630593964.8438, tv loss=23438880.0000\n",
            "L-BFGS | iteration: 275, total loss=1612656128.0000, content_loss=960629687.5000, style loss=628590761.7188, tv loss=23435648.0000\n",
            "L-BFGS | iteration: 276, total loss=1610794240.0000, content_loss=960443359.3750, style loss=626912519.5312, tv loss=23438360.0000\n",
            "L-BFGS | iteration: 277, total loss=1606573696.0000, content_loss=959776171.8750, style loss=623352363.2812, tv loss=23445090.0000\n",
            "L-BFGS | iteration: 278, total loss=1604619392.0000, content_loss=959679101.5625, style loss=621477773.4375, tv loss=23462530.0000\n",
            "L-BFGS | iteration: 279, total loss=1601373312.0000, content_loss=959648828.1250, style loss=618271230.4688, tv loss=23453282.0000\n",
            "L-BFGS | iteration: 280, total loss=1599537536.0000, content_loss=958883789.0625, style loss=617192753.9062, tv loss=23461032.0000\n",
            "L-BFGS | iteration: 281, total loss=1597855744.0000, content_loss=958903125.0000, style loss=615491425.7812, tv loss=23461056.0000\n",
            "L-BFGS | iteration: 282, total loss=1595191040.0000, content_loss=958507324.2188, style loss=613219160.1562, tv loss=23464628.0000\n",
            "L-BFGS | iteration: 283, total loss=1591828992.0000, content_loss=957873242.1875, style loss=610482128.9062, tv loss=23473696.0000\n",
            "L-BFGS | iteration: 284, total loss=1589232768.0000, content_loss=956874804.6875, style loss=608890957.0312, tv loss=23467020.0000\n",
            "L-BFGS | iteration: 285, total loss=1587720448.0000, content_loss=956783789.0625, style loss=607471523.4375, tv loss=23465070.0000\n",
            "L-BFGS | iteration: 286, total loss=1585920000.0000, content_loss=955920214.8438, style loss=606524648.4375, tv loss=23475178.0000\n",
            "L-BFGS | iteration: 287, total loss=1583721728.0000, content_loss=955787792.9688, style loss=604458457.0312, tv loss=23475476.0000\n",
            "L-BFGS | iteration: 288, total loss=1580867456.0000, content_loss=955426074.2188, style loss=601960312.5000, tv loss=23481036.0000\n",
            "L-BFGS | iteration: 289, total loss=1577474688.0000, content_loss=954635644.5312, style loss=599347675.7812, tv loss=23491424.0000\n",
            "L-BFGS | iteration: 290, total loss=1575034496.0000, content_loss=954831250.0000, style loss=596708964.8438, tv loss=23494220.0000\n",
            "L-BFGS | iteration: 291, total loss=1573340928.0000, content_loss=954613574.2188, style loss=595231171.8750, tv loss=23496180.0000\n",
            "L-BFGS | iteration: 292, total loss=1570497024.0000, content_loss=954467675.7812, style loss=592532812.5000, tv loss=23496560.0000\n",
            "L-BFGS | iteration: 293, total loss=1568452352.0000, content_loss=954090332.0312, style loss=590853574.2188, tv loss=23508432.0000\n",
            "L-BFGS | iteration: 294, total loss=1565583232.0000, content_loss=954145800.7812, style loss=587930859.3750, tv loss=23506620.0000\n",
            "L-BFGS | iteration: 295, total loss=1563733760.0000, content_loss=953109570.3125, style loss=587116699.2188, tv loss=23507482.0000\n",
            "L-BFGS | iteration: 296, total loss=1562126720.0000, content_loss=952805761.7188, style loss=585815156.2500, tv loss=23505822.0000\n",
            "L-BFGS | iteration: 297, total loss=1559534976.0000, content_loss=952086230.4688, style loss=583938808.5938, tv loss=23509860.0000\n",
            "L-BFGS | iteration: 298, total loss=1556858112.0000, content_loss=951413183.5938, style loss=581936542.9688, tv loss=23508428.0000\n",
            "L-BFGS | iteration: 299, total loss=1554691328.0000, content_loss=950358398.4375, style loss=580815117.1875, tv loss=23517772.0000\n",
            "L-BFGS | iteration: 300, total loss=1553087616.0000, content_loss=950794433.5938, style loss=578777988.2812, tv loss=23515212.0000\n",
            "L-BFGS | iteration: 301, total loss=1551380736.0000, content_loss=950753515.6250, style loss=577107714.8438, tv loss=23519466.0000\n",
            "L-BFGS | iteration: 302, total loss=1550016256.0000, content_loss=950601269.5312, style loss=575893300.7812, tv loss=23521628.0000\n",
            "L-BFGS | iteration: 303, total loss=1547844480.0000, content_loss=950520898.4375, style loss=573798046.8750, tv loss=23525540.0000\n",
            "L-BFGS | iteration: 304, total loss=1545004672.0000, content_loss=950060253.9062, style loss=571417675.7812, tv loss=23526840.0000\n",
            "L-BFGS | iteration: 305, total loss=1542426496.0000, content_loss=949815917.9688, style loss=569075332.0312, tv loss=23535248.0000\n",
            "L-BFGS | iteration: 306, total loss=1540720896.0000, content_loss=949588476.5625, style loss=567599414.0625, tv loss=23533058.0000\n",
            "L-BFGS | iteration: 307, total loss=1539192064.0000, content_loss=948893359.3750, style loss=566757246.0938, tv loss=23541450.0000\n",
            "L-BFGS | iteration: 308, total loss=1536934912.0000, content_loss=948818066.4062, style loss=564576738.2812, tv loss=23540072.0000\n",
            "L-BFGS | iteration: 309, total loss=1535050624.0000, content_loss=948303320.3125, style loss=563207285.1562, tv loss=23540082.0000\n",
            "L-BFGS | iteration: 310, total loss=1533208448.0000, content_loss=947983789.0625, style loss=561684257.8125, tv loss=23540360.0000\n",
            "L-BFGS | iteration: 311, total loss=1531157376.0000, content_loss=947267285.1562, style loss=560346914.0625, tv loss=23543130.0000\n",
            "L-BFGS | iteration: 312, total loss=1528815360.0000, content_loss=946962109.3750, style loss=558311132.8125, tv loss=23542176.0000\n",
            "L-BFGS | iteration: 313, total loss=1527073280.0000, content_loss=946358300.7812, style loss=557167324.2188, tv loss=23547612.0000\n",
            "L-BFGS | iteration: 314, total loss=1525233920.0000, content_loss=946121484.3750, style loss=555565664.0625, tv loss=23546776.0000\n",
            "L-BFGS | iteration: 315, total loss=1523541248.0000, content_loss=945613183.5938, style loss=554375566.4062, tv loss=23552536.0000\n",
            "L-BFGS | iteration: 316, total loss=1521424256.0000, content_loss=945391308.5938, style loss=552477363.2812, tv loss=23555628.0000\n",
            "L-BFGS | iteration: 317, total loss=1519253632.0000, content_loss=945110839.8438, style loss=550584550.7812, tv loss=23558258.0000\n",
            "L-BFGS | iteration: 318, total loss=1517142656.0000, content_loss=944851855.4688, style loss=548734921.8750, tv loss=23556024.0000\n",
            "L-BFGS | iteration: 319, total loss=1515633792.0000, content_loss=944482421.8750, style loss=547592812.5000, tv loss=23558498.0000\n",
            "L-BFGS | iteration: 320, total loss=1514140800.0000, content_loss=944374218.7500, style loss=546209238.2812, tv loss=23557366.0000\n",
            "L-BFGS | iteration: 321, total loss=1512908800.0000, content_loss=943756152.3438, style loss=545590078.1250, tv loss=23562524.0000\n",
            "L-BFGS | iteration: 322, total loss=1511193344.0000, content_loss=943876367.1875, style loss=543754980.4688, tv loss=23561994.0000\n",
            "L-BFGS | iteration: 323, total loss=1509398144.0000, content_loss=943822753.9062, style loss=542011582.0312, tv loss=23563764.0000\n",
            "L-BFGS | iteration: 324, total loss=1507142528.0000, content_loss=943623535.1562, style loss=539950429.6875, tv loss=23568572.0000\n",
            "L-BFGS | iteration: 325, total loss=1505241216.0000, content_loss=943201367.1875, style loss=538474628.9062, tv loss=23565240.0000\n",
            "L-BFGS | iteration: 326, total loss=1503952128.0000, content_loss=943049414.0625, style loss=537337792.9688, tv loss=23564870.0000\n",
            "L-BFGS | iteration: 327, total loss=1502526976.0000, content_loss=942109277.3438, style loss=536849179.6875, tv loss=23568498.0000\n",
            "L-BFGS | iteration: 328, total loss=1501051136.0000, content_loss=941918261.7188, style loss=535569550.7812, tv loss=23563284.0000\n",
            "L-BFGS | iteration: 329, total loss=1499492608.0000, content_loss=941591113.2812, style loss=534334980.4688, tv loss=23566634.0000\n",
            "L-BFGS | iteration: 330, total loss=1497542784.0000, content_loss=940692089.8438, style loss=533279121.0938, tv loss=23571556.0000\n",
            "L-BFGS | iteration: 331, total loss=1496427648.0000, content_loss=941000097.6562, style loss=531857988.2812, tv loss=23569592.0000\n",
            "L-BFGS | iteration: 332, total loss=1495484416.0000, content_loss=940949609.3750, style loss=530966660.1562, tv loss=23568152.0000\n",
            "L-BFGS | iteration: 333, total loss=1492880768.0000, content_loss=940441601.5625, style loss=528873046.8750, tv loss=23566134.0000\n",
            "L-BFGS | iteration: 334, total loss=1490914432.0000, content_loss=940120507.8125, style loss=527228027.3438, tv loss=23565960.0000\n",
            "L-BFGS | iteration: 335, total loss=1489223680.0000, content_loss=939872851.5625, style loss=525788496.0938, tv loss=23562400.0000\n",
            "L-BFGS | iteration: 336, total loss=1488102272.0000, content_loss=939304785.1562, style loss=525230976.5625, tv loss=23566500.0000\n",
            "L-BFGS | iteration: 337, total loss=1487030272.0000, content_loss=939286230.4688, style loss=524177929.6875, tv loss=23566116.0000\n",
            "L-BFGS | iteration: 338, total loss=1485739648.0000, content_loss=938822070.3125, style loss=523353046.8750, tv loss=23564482.0000\n",
            "L-BFGS | iteration: 339, total loss=1484063488.0000, content_loss=938528027.3438, style loss=521968652.3438, tv loss=23566838.0000\n",
            "L-BFGS | iteration: 340, total loss=1482401664.0000, content_loss=937901660.1562, style loss=520934882.8125, tv loss=23565204.0000\n",
            "L-BFGS | iteration: 341, total loss=1480727424.0000, content_loss=937780175.7812, style loss=519380449.2188, tv loss=23566836.0000\n",
            "L-BFGS | iteration: 342, total loss=1479315712.0000, content_loss=937516015.6250, style loss=518230722.6562, tv loss=23568882.0000\n",
            "L-BFGS | iteration: 343, total loss=1477520000.0000, content_loss=937075000.0000, style loss=516878320.3125, tv loss=23566688.0000\n",
            "L-BFGS | iteration: 344, total loss=1476479104.0000, content_loss=936249316.4062, style loss=516660410.1562, tv loss=23569336.0000\n",
            "L-BFGS | iteration: 345, total loss=1475452800.0000, content_loss=936354394.5312, style loss=515532187.5000, tv loss=23566176.0000\n",
            "L-BFGS | iteration: 346, total loss=1474455936.0000, content_loss=936200781.2500, style loss=514686738.2812, tv loss=23568332.0000\n",
            "L-BFGS | iteration: 347, total loss=1473582976.0000, content_loss=935907714.8438, style loss=514107832.0312, tv loss=23567496.0000\n",
            "L-BFGS | iteration: 348, total loss=1472561664.0000, content_loss=935699609.3750, style loss=513295781.2500, tv loss=23566308.0000\n",
            "L-BFGS | iteration: 349, total loss=1470358528.0000, content_loss=935281054.6875, style loss=511510253.9062, tv loss=23567264.0000\n",
            "L-BFGS | iteration: 350, total loss=1469115904.0000, content_loss=935009472.6562, style loss=510545273.4375, tv loss=23561278.0000\n",
            "L-BFGS | iteration: 351, total loss=1467693568.0000, content_loss=935108984.3750, style loss=509020898.4375, tv loss=23563598.0000\n",
            "L-BFGS | iteration: 352, total loss=1466587648.0000, content_loss=934831738.2812, style loss=508187343.7500, tv loss=23568688.0000\n",
            "L-BFGS | iteration: 353, total loss=1465521152.0000, content_loss=935042871.0938, style loss=506910175.7812, tv loss=23568074.0000\n",
            "L-BFGS | iteration: 354, total loss=1464325248.0000, content_loss=934805175.7812, style loss=505951347.6562, tv loss=23568818.0000\n",
            "L-BFGS | iteration: 355, total loss=1462715776.0000, content_loss=934399121.0938, style loss=504747949.2188, tv loss=23568736.0000\n",
            "L-BFGS | iteration: 356, total loss=1461539968.0000, content_loss=934348828.1250, style loss=503625410.1562, tv loss=23565756.0000\n",
            "L-BFGS | iteration: 357, total loss=1459939968.0000, content_loss=933940917.9688, style loss=502434316.4062, tv loss=23564652.0000\n",
            "L-BFGS | iteration: 358, total loss=1458467968.0000, content_loss=933699804.6875, style loss=501203378.9062, tv loss=23564760.0000\n",
            "L-BFGS | iteration: 359, total loss=1457197824.0000, content_loss=933184082.0312, style loss=500451738.2812, tv loss=23561950.0000\n",
            "L-BFGS | iteration: 360, total loss=1456037504.0000, content_loss=932535253.9062, style loss=499939335.9375, tv loss=23562890.0000\n",
            "L-BFGS | iteration: 361, total loss=1454748032.0000, content_loss=932222753.9062, style loss=498963984.3750, tv loss=23561262.0000\n",
            "L-BFGS | iteration: 362, total loss=1453743872.0000, content_loss=931894335.9375, style loss=498289335.9375, tv loss=23560160.0000\n",
            "L-BFGS | iteration: 363, total loss=1452608512.0000, content_loss=931871972.6562, style loss=497177226.5625, tv loss=23559360.0000\n",
            "L-BFGS | iteration: 364, total loss=1451185280.0000, content_loss=931395507.8125, style loss=496231992.1875, tv loss=23557792.0000\n",
            "L-BFGS | iteration: 365, total loss=1449683840.0000, content_loss=931429687.5000, style loss=494700175.7812, tv loss=23553860.0000\n",
            "L-BFGS | iteration: 366, total loss=1448537728.0000, content_loss=931056152.3438, style loss=493925507.8125, tv loss=23556152.0000\n",
            "L-BFGS | iteration: 367, total loss=1447534976.0000, content_loss=931050585.9375, style loss=492930351.5625, tv loss=23554096.0000\n",
            "L-BFGS | iteration: 368, total loss=1446255488.0000, content_loss=930883984.3750, style loss=491820117.1875, tv loss=23551416.0000\n",
            "L-BFGS | iteration: 369, total loss=1444762496.0000, content_loss=930773535.1562, style loss=490439970.7031, tv loss=23549116.0000\n",
            "L-BFGS | iteration: 370, total loss=1443546624.0000, content_loss=930464160.1562, style loss=489534697.2656, tv loss=23547820.0000\n",
            "L-BFGS | iteration: 371, total loss=1442797568.0000, content_loss=930500585.9375, style loss=488751621.0938, tv loss=23545392.0000\n",
            "L-BFGS | iteration: 372, total loss=1441933824.0000, content_loss=930013574.2188, style loss=488373574.2188, tv loss=23546688.0000\n",
            "L-BFGS | iteration: 373, total loss=1441214720.0000, content_loss=929919726.5625, style loss=487748203.1250, tv loss=23546730.0000\n",
            "L-BFGS | iteration: 374, total loss=1440156544.0000, content_loss=929488867.1875, style loss=487122480.4688, tv loss=23545232.0000\n",
            "L-BFGS | iteration: 375, total loss=1439067520.0000, content_loss=929169433.5938, style loss=486354667.9688, tv loss=23543468.0000\n",
            "L-BFGS | iteration: 376, total loss=1437743872.0000, content_loss=927992578.1250, style loss=486214453.1250, tv loss=23536896.0000\n",
            "L-BFGS | iteration: 377, total loss=1436536448.0000, content_loss=928205371.0938, style loss=484793818.3594, tv loss=23537324.0000\n",
            "L-BFGS | iteration: 378, total loss=1435881856.0000, content_loss=928147167.9688, style loss=484197392.5781, tv loss=23537248.0000\n",
            "L-BFGS | iteration: 379, total loss=1434977920.0000, content_loss=928010839.8438, style loss=483433271.4844, tv loss=23533776.0000\n",
            "L-BFGS | iteration: 380, total loss=1433710976.0000, content_loss=927517578.1250, style loss=482661533.2031, tv loss=23531850.0000\n",
            "L-BFGS | iteration: 381, total loss=1432379904.0000, content_loss=927387792.9688, style loss=481464375.0000, tv loss=23527622.0000\n",
            "L-BFGS | iteration: 382, total loss=1431446144.0000, content_loss=927199804.6875, style loss=480719033.2031, tv loss=23527350.0000\n",
            "L-BFGS | iteration: 383, total loss=1430088192.0000, content_loss=927181542.9688, style loss=479382421.8750, tv loss=23524204.0000\n",
            "L-BFGS | iteration: 384, total loss=1429005952.0000, content_loss=927004687.5000, style loss=478475683.5938, tv loss=23525588.0000\n",
            "L-BFGS | iteration: 385, total loss=1428229376.0000, content_loss=926991601.5625, style loss=477713056.6406, tv loss=23524764.0000\n",
            "L-BFGS | iteration: 386, total loss=1426972288.0000, content_loss=926835058.5938, style loss=476614921.8750, tv loss=23522256.0000\n",
            "L-BFGS | iteration: 387, total loss=1425996544.0000, content_loss=926477441.4062, style loss=475997958.9844, tv loss=23521188.0000\n",
            "L-BFGS | iteration: 388, total loss=1424814080.0000, content_loss=926076367.1875, style loss=475222089.8438, tv loss=23515680.0000\n",
            "L-BFGS | iteration: 389, total loss=1424207232.0000, content_loss=925383398.4375, style loss=475309423.8281, tv loss=23514346.0000\n",
            "L-BFGS | iteration: 390, total loss=1423355520.0000, content_loss=925414648.4375, style loss=474428994.1406, tv loss=23511912.0000\n",
            "L-BFGS | iteration: 391, total loss=1422690048.0000, content_loss=925314941.4062, style loss=473864062.5000, tv loss=23511052.0000\n",
            "L-BFGS | iteration: 392, total loss=1421822592.0000, content_loss=925109667.9688, style loss=473203212.8906, tv loss=23509630.0000\n",
            "L-BFGS | iteration: 393, total loss=1421082880.0000, content_loss=924991015.6250, style loss=472583408.2031, tv loss=23508500.0000\n",
            "L-BFGS | iteration: 394, total loss=1419388800.0000, content_loss=924400585.9375, style loss=471485156.2500, tv loss=23503154.0000\n",
            "L-BFGS | iteration: 395, total loss=1418253952.0000, content_loss=924515722.6562, style loss=470234824.2188, tv loss=23503480.0000\n",
            "L-BFGS | iteration: 396, total loss=1417458944.0000, content_loss=924226269.5312, style loss=469734111.3281, tv loss=23498602.0000\n",
            "L-BFGS | iteration: 397, total loss=1416653568.0000, content_loss=923957714.8438, style loss=469203339.8438, tv loss=23492582.0000\n",
            "L-BFGS | iteration: 398, total loss=1415934848.0000, content_loss=923686816.4062, style loss=468759111.3281, tv loss=23488888.0000\n",
            "L-BFGS | iteration: 399, total loss=1414907264.0000, content_loss=923706835.9375, style loss=467719423.8281, tv loss=23481018.0000\n",
            "L-BFGS | iteration: 400, total loss=1413933056.0000, content_loss=923354296.8750, style loss=467097011.7188, tv loss=23481744.0000\n",
            "L-BFGS | iteration: 401, total loss=1412999936.0000, content_loss=923406835.9375, style loss=466108593.7500, tv loss=23484466.0000\n",
            "L-BFGS | iteration: 402, total loss=1412143616.0000, content_loss=923100000.0000, style loss=465558955.0781, tv loss=23484732.0000\n",
            "L-BFGS | iteration: 403, total loss=1411490816.0000, content_loss=922898535.1562, style loss=465108046.8750, tv loss=23484126.0000\n",
            "L-BFGS | iteration: 404, total loss=1410334208.0000, content_loss=922531347.6562, style loss=464321074.2188, tv loss=23481856.0000\n",
            "L-BFGS | iteration: 405, total loss=1409173376.0000, content_loss=922388964.8438, style loss=463305292.9688, tv loss=23479200.0000\n",
            "L-BFGS | iteration: 406, total loss=1408403584.0000, content_loss=922311425.7812, style loss=462616230.4688, tv loss=23475920.0000\n",
            "L-BFGS | iteration: 407, total loss=1407292672.0000, content_loss=922061718.7500, style loss=461761025.3906, tv loss=23469916.0000\n",
            "L-BFGS | iteration: 408, total loss=1406427392.0000, content_loss=921794140.6250, style loss=461166767.5781, tv loss=23466488.0000\n",
            "L-BFGS | iteration: 409, total loss=1405469952.0000, content_loss=921702441.4062, style loss=460304619.1406, tv loss=23462970.0000\n",
            "L-BFGS | iteration: 410, total loss=1405007488.0000, content_loss=921092871.0938, style loss=460454443.3594, tv loss=23460228.0000\n",
            "L-BFGS | iteration: 411, total loss=1404406400.0000, content_loss=921126660.1562, style loss=459818730.4688, tv loss=23461052.0000\n",
            "L-BFGS | iteration: 412, total loss=1403825792.0000, content_loss=921065429.6875, style loss=459300058.5938, tv loss=23460384.0000\n",
            "L-BFGS | iteration: 413, total loss=1402921984.0000, content_loss=920870507.8125, style loss=458593417.9688, tv loss=23458064.0000\n",
            "L-BFGS | iteration: 414, total loss=1401754752.0000, content_loss=920732519.5312, style loss=457567734.3750, tv loss=23454428.0000\n",
            "L-BFGS | iteration: 415, total loss=1400669056.0000, content_loss=920171777.3438, style loss=457051142.5781, tv loss=23446188.0000\n",
            "L-BFGS | iteration: 416, total loss=1399770752.0000, content_loss=920375781.2500, style loss=455951630.8594, tv loss=23443350.0000\n",
            "L-BFGS | iteration: 417, total loss=1399224832.0000, content_loss=920159375.0000, style loss=455624677.7344, tv loss=23440754.0000\n",
            "L-BFGS | iteration: 418, total loss=1398528768.0000, content_loss=919935253.9062, style loss=455154931.6406, tv loss=23438646.0000\n",
            "L-BFGS | iteration: 419, total loss=1397689984.0000, content_loss=919564648.4375, style loss=454689462.8906, tv loss=23435858.0000\n",
            "L-BFGS | iteration: 420, total loss=1396708992.0000, content_loss=919388085.9375, style loss=453887314.4531, tv loss=23433588.0000\n",
            "L-BFGS | iteration: 421, total loss=1395881344.0000, content_loss=919098339.8438, style loss=453352500.0000, tv loss=23430508.0000\n",
            "L-BFGS | iteration: 422, total loss=1394954368.0000, content_loss=918960937.5000, style loss=452566142.5781, tv loss=23427138.0000\n",
            "L-BFGS | iteration: 423, total loss=1393991808.0000, content_loss=918775292.9688, style loss=451795781.2500, tv loss=23420792.0000\n",
            "L-BFGS | iteration: 424, total loss=1393322752.0000, content_loss=918239843.7500, style loss=451666142.5781, tv loss=23416666.0000\n",
            "L-BFGS | iteration: 425, total loss=1392598528.0000, content_loss=918269433.5938, style loss=450914121.0938, tv loss=23414980.0000\n",
            "L-BFGS | iteration: 426, total loss=1392012800.0000, content_loss=918111621.0938, style loss=450490136.7188, tv loss=23411112.0000\n",
            "L-BFGS | iteration: 427, total loss=1391550336.0000, content_loss=917943261.7188, style loss=450197314.4531, tv loss=23409756.0000\n",
            "L-BFGS | iteration: 428, total loss=1390815872.0000, content_loss=917734960.9375, style loss=449673515.6250, tv loss=23407304.0000\n",
            "L-BFGS | iteration: 429, total loss=1389774976.0000, content_loss=917575878.9062, style loss=448795078.1250, tv loss=23403970.0000\n",
            "L-BFGS | iteration: 430, total loss=1388875008.0000, content_loss=917295019.5312, style loss=448181630.8594, tv loss=23398424.0000\n",
            "L-BFGS | iteration: 431, total loss=1388122240.0000, content_loss=917318261.7188, style loss=447407578.1250, tv loss=23396360.0000\n",
            "L-BFGS | iteration: 432, total loss=1387408512.0000, content_loss=917085156.2500, style loss=446931005.8594, tv loss=23392444.0000\n",
            "L-BFGS | iteration: 433, total loss=1386432640.0000, content_loss=916783593.7500, style loss=446261250.0000, tv loss=23387806.0000\n",
            "L-BFGS | iteration: 434, total loss=1385605760.0000, content_loss=916488476.5625, style loss=445733730.4688, tv loss=23383548.0000\n",
            "L-BFGS | iteration: 435, total loss=1385038336.0000, content_loss=916323144.5312, style loss=445334062.5000, tv loss=23381182.0000\n",
            "L-BFGS | iteration: 436, total loss=1384570240.0000, content_loss=916410546.8750, style loss=444781142.5781, tv loss=23378604.0000\n",
            "L-BFGS | iteration: 437, total loss=1383862400.0000, content_loss=916157617.1875, style loss=444327978.5156, tv loss=23376800.0000\n",
            "L-BFGS | iteration: 438, total loss=1383255424.0000, content_loss=916047656.2500, style loss=443832802.7344, tv loss=23374980.0000\n",
            "L-BFGS | iteration: 439, total loss=1382471936.0000, content_loss=915869921.8750, style loss=443230546.8750, tv loss=23371412.0000\n",
            "L-BFGS | iteration: 440, total loss=1381546752.0000, content_loss=915483105.4688, style loss=442698720.7031, tv loss=23364900.0000\n",
            "L-BFGS | iteration: 441, total loss=1380726016.0000, content_loss=915358398.4375, style loss=442006845.7031, tv loss=23360800.0000\n",
            "L-BFGS | iteration: 442, total loss=1380175616.0000, content_loss=915180761.7188, style loss=441637558.5938, tv loss=23357312.0000\n",
            "L-BFGS | iteration: 443, total loss=1379365632.0000, content_loss=914871679.6875, style loss=441142089.8438, tv loss=23351766.0000\n",
            "L-BFGS | iteration: 444, total loss=1378243712.0000, content_loss=914536328.1250, style loss=440364902.3438, tv loss=23342436.0000\n",
            "L-BFGS | iteration: 445, total loss=1377781504.0000, content_loss=914089746.0938, style loss=440357021.4844, tv loss=23334842.0000\n",
            "L-BFGS | iteration: 446, total loss=1377245056.0000, content_loss=914396386.7188, style loss=439511777.3438, tv loss=23336860.0000\n",
            "L-BFGS | iteration: 447, total loss=1376783232.0000, content_loss=914373730.4688, style loss=439072792.9688, tv loss=23336660.0000\n",
            "L-BFGS | iteration: 448, total loss=1376413568.0000, content_loss=914239550.7812, style loss=438838974.6094, tv loss=23334996.0000\n",
            "L-BFGS | iteration: 449, total loss=1375857408.0000, content_loss=914028320.3125, style loss=438496494.1406, tv loss=23332624.0000\n",
            "L-BFGS | iteration: 450, total loss=1375038848.0000, content_loss=913792382.8125, style loss=437916708.9844, tv loss=23329806.0000\n",
            "L-BFGS | iteration: 451, total loss=1373916544.0000, content_loss=913565820.3125, style loss=437027197.2656, tv loss=23323546.0000\n",
            "L-BFGS | iteration: 452, total loss=1373156224.0000, content_loss=913316015.6250, style loss=436520244.1406, tv loss=23319988.0000\n",
            "L-BFGS | iteration: 453, total loss=1372622720.0000, content_loss=913253125.0000, style loss=436052666.0156, tv loss=23316816.0000\n",
            "L-BFGS | iteration: 454, total loss=1372036480.0000, content_loss=913086816.4062, style loss=435638759.7656, tv loss=23310876.0000\n",
            "L-BFGS | iteration: 455, total loss=1371219456.0000, content_loss=912766210.9375, style loss=435147099.6094, tv loss=23306102.0000\n",
            "L-BFGS | iteration: 456, total loss=1370448384.0000, content_loss=912442382.8125, style loss=434712041.0156, tv loss=23293956.0000\n",
            "L-BFGS | iteration: 457, total loss=1369819648.0000, content_loss=912346875.0000, style loss=434178193.3594, tv loss=23294548.0000\n",
            "L-BFGS | iteration: 458, total loss=1369275392.0000, content_loss=912251171.8750, style loss=433729453.1250, tv loss=23294740.0000\n",
            "L-BFGS | iteration: 459, total loss=1368903168.0000, content_loss=911868457.0312, style loss=433746386.7188, tv loss=23288344.0000\n",
            "L-BFGS | iteration: 460, total loss=1368265984.0000, content_loss=912048535.1562, style loss=432929355.4688, tv loss=23288048.0000\n",
            "L-BFGS | iteration: 461, total loss=1367812736.0000, content_loss=912000195.3125, style loss=432527314.4531, tv loss=23285192.0000\n",
            "L-BFGS | iteration: 462, total loss=1367032704.0000, content_loss=911839257.8125, style loss=431916650.3906, tv loss=23276760.0000\n",
            "L-BFGS | iteration: 463, total loss=1366248704.0000, content_loss=911746875.0000, style loss=431228906.2500, tv loss=23272932.0000\n",
            "L-BFGS | iteration: 464, total loss=1365433344.0000, content_loss=911501855.4688, style loss=430664736.3281, tv loss=23266768.0000\n",
            "L-BFGS | iteration: 465, total loss=1364850176.0000, content_loss=911327929.6875, style loss=430258505.8594, tv loss=23263766.0000\n",
            "L-BFGS | iteration: 466, total loss=1364273792.0000, content_loss=911006250.0000, style loss=430009833.9844, tv loss=23257748.0000\n",
            "L-BFGS | iteration: 467, total loss=1363519872.0000, content_loss=910837011.7188, style loss=429430283.2031, tv loss=23252552.0000\n",
            "L-BFGS | iteration: 468, total loss=1362921216.0000, content_loss=910429687.5000, style loss=429245771.4844, tv loss=23245882.0000\n",
            "L-BFGS | iteration: 469, total loss=1362370816.0000, content_loss=910465917.9688, style loss=428660244.1406, tv loss=23244644.0000\n",
            "L-BFGS | iteration: 470, total loss=1361913472.0000, content_loss=910419140.6250, style loss=428253808.5938, tv loss=23240516.0000\n",
            "L-BFGS | iteration: 471, total loss=1361261952.0000, content_loss=910324707.0312, style loss=427701240.2344, tv loss=23235980.0000\n",
            "L-BFGS | iteration: 472, total loss=1360535680.0000, content_loss=910012109.3750, style loss=427294423.8281, tv loss=23229174.0000\n",
            "L-BFGS | iteration: 473, total loss=1359916288.0000, content_loss=910026171.8750, style loss=426663603.5156, tv loss=23226454.0000\n",
            "L-BFGS | iteration: 474, total loss=1359505920.0000, content_loss=909894531.2500, style loss=426387861.3281, tv loss=23223582.0000\n",
            "L-BFGS | iteration: 475, total loss=1358799744.0000, content_loss=909723632.8125, style loss=425856650.3906, tv loss=23219464.0000\n",
            "L-BFGS | iteration: 476, total loss=1358298368.0000, content_loss=909389355.4688, style loss=425698359.3750, tv loss=23210620.0000\n",
            "L-BFGS | iteration: 477, total loss=1357685120.0000, content_loss=909433789.0625, style loss=425040322.2656, tv loss=23210976.0000\n",
            "L-BFGS | iteration: 478, total loss=1357289344.0000, content_loss=909313378.9062, style loss=424767861.3281, tv loss=23208088.0000\n",
            "L-BFGS | iteration: 479, total loss=1356893056.0000, content_loss=909168164.0625, style loss=424520302.7344, tv loss=23204568.0000\n",
            "L-BFGS | iteration: 480, total loss=1356171776.0000, content_loss=908858105.4688, style loss=424117236.3281, tv loss=23196476.0000\n",
            "L-BFGS | iteration: 481, total loss=1355488000.0000, content_loss=908564257.8125, style loss=423733593.7500, tv loss=23190094.0000\n",
            "L-BFGS | iteration: 482, total loss=1355026944.0000, content_loss=908503613.2812, style loss=423334892.5781, tv loss=23188448.0000\n",
            "L-BFGS | iteration: 483, total loss=1354565248.0000, content_loss=908257617.1875, style loss=423123603.5156, tv loss=23184038.0000\n",
            "L-BFGS | iteration: 484, total loss=1353968640.0000, content_loss=908235156.2500, style loss=422552255.8594, tv loss=23181364.0000\n",
            "L-BFGS | iteration: 485, total loss=1353297280.0000, content_loss=907805078.1250, style loss=422319345.7031, tv loss=23172882.0000\n",
            "L-BFGS | iteration: 486, total loss=1352746240.0000, content_loss=907821386.7188, style loss=421754121.0938, tv loss=23170824.0000\n",
            "L-BFGS | iteration: 487, total loss=1352322048.0000, content_loss=907810839.8438, style loss=421344697.2656, tv loss=23166430.0000\n",
            "L-BFGS | iteration: 488, total loss=1351808768.0000, content_loss=907726464.8438, style loss=420919072.2656, tv loss=23163316.0000\n",
            "L-BFGS | iteration: 489, total loss=1350959232.0000, content_loss=907490039.0625, style loss=420312011.7188, tv loss=23157156.0000\n",
            "L-BFGS | iteration: 490, total loss=1350533632.0000, content_loss=907377734.3750, style loss=420009345.7031, tv loss=23146548.0000\n",
            "L-BFGS | iteration: 491, total loss=1349986688.0000, content_loss=907326464.8438, style loss=419512792.9688, tv loss=23147348.0000\n",
            "L-BFGS | iteration: 492, total loss=1349523456.0000, content_loss=907244531.2500, style loss=419133017.5781, tv loss=23145864.0000\n",
            "L-BFGS | iteration: 493, total loss=1349066624.0000, content_loss=907104492.1875, style loss=418819892.5781, tv loss=23142270.0000\n",
            "L-BFGS | iteration: 494, total loss=1348188672.0000, content_loss=906842578.1250, style loss=418212626.9531, tv loss=23133494.0000\n",
            "L-BFGS | iteration: 495, total loss=1347580672.0000, content_loss=906347167.9688, style loss=418108798.8281, tv loss=23124694.0000\n",
            "L-BFGS | iteration: 496, total loss=1347271424.0000, content_loss=906370996.0938, style loss=417776542.9688, tv loss=23123866.0000\n",
            "L-BFGS | iteration: 497, total loss=1346784000.0000, content_loss=906206347.6562, style loss=417459755.8594, tv loss=23117942.0000\n",
            "L-BFGS | iteration: 498, total loss=1346545024.0000, content_loss=906130566.4062, style loss=417296689.4531, tv loss=23117734.0000\n",
            "L-BFGS | iteration: 499, total loss=1346034560.0000, content_loss=905960839.8438, style loss=416958134.7656, tv loss=23115696.0000\n",
            "L-BFGS | iteration: 500, total loss=1345442304.0000, content_loss=905844628.9062, style loss=416486718.7500, tv loss=23110938.0000\n",
            "L-BFGS | iteration: 501, total loss=1344815488.0000, content_loss=905721386.7188, style loss=415991689.4531, tv loss=23102302.0000\n",
            "L-BFGS | iteration: 502, total loss=1344323328.0000, content_loss=905734960.9375, style loss=415491005.8594, tv loss=23097284.0000\n",
            "L-BFGS | iteration: 503, total loss=1343914112.0000, content_loss=905594335.9375, style loss=415229267.5781, tv loss=23090544.0000\n",
            "L-BFGS | iteration: 504, total loss=1343213568.0000, content_loss=905482519.5312, style loss=414648750.0000, tv loss=23082426.0000\n",
            "L-BFGS | iteration: 505, total loss=1342734848.0000, content_loss=905077441.4062, style loss=414583564.4531, tv loss=23073798.0000\n",
            "L-BFGS | iteration: 506, total loss=1342173952.0000, content_loss=905014648.4375, style loss=414084052.7344, tv loss=23075206.0000\n",
            "L-BFGS | iteration: 507, total loss=1341762560.0000, content_loss=904736718.7500, style loss=413953359.3750, tv loss=23072448.0000\n",
            "L-BFGS | iteration: 508, total loss=1341437696.0000, content_loss=904617773.4375, style loss=413748339.8438, tv loss=23071508.0000\n",
            "L-BFGS | iteration: 509, total loss=1340908544.0000, content_loss=904382226.5625, style loss=413458564.4531, tv loss=23067808.0000\n",
            "L-BFGS | iteration: 510, total loss=1340214144.0000, content_loss=904137988.2812, style loss=413014775.3906, tv loss=23061326.0000\n",
            "L-BFGS | iteration: 511, total loss=1339653120.0000, content_loss=903990332.0312, style loss=412608046.8750, tv loss=23054700.0000\n",
            "L-BFGS | iteration: 512, total loss=1339254016.0000, content_loss=904094140.6250, style loss=412108359.3750, tv loss=23051576.0000\n",
            "L-BFGS | iteration: 513, total loss=1338862464.0000, content_loss=903950781.2500, style loss=411867568.3594, tv loss=23044114.0000\n",
            "L-BFGS | iteration: 514, total loss=1338389248.0000, content_loss=903976953.1250, style loss=411372187.5000, tv loss=23040152.0000\n",
            "L-BFGS | iteration: 515, total loss=1337907072.0000, content_loss=903782714.8438, style loss=411090849.6094, tv loss=23033444.0000\n",
            "L-BFGS | iteration: 516, total loss=1337299968.0000, content_loss=903553222.6562, style loss=410720361.3281, tv loss=23026492.0000\n",
            "L-BFGS | iteration: 517, total loss=1336785024.0000, content_loss=903491699.2188, style loss=410271298.8281, tv loss=23021892.0000\n",
            "L-BFGS | iteration: 518, total loss=1336224896.0000, content_loss=903279785.1562, style loss=409928583.9844, tv loss=23016532.0000\n",
            "L-BFGS | iteration: 519, total loss=1335789184.0000, content_loss=903218457.0312, style loss=409557861.3281, tv loss=23012818.0000\n",
            "L-BFGS | iteration: 520, total loss=1335221248.0000, content_loss=903092773.4375, style loss=409120693.3594, tv loss=23007800.0000\n",
            "L-BFGS | iteration: 521, total loss=1334799616.0000, content_loss=902812500.0000, style loss=408986542.9688, tv loss=23000532.0000\n",
            "L-BFGS | iteration: 522, total loss=1334369792.0000, content_loss=902764746.0938, style loss=408604775.3906, tv loss=23000268.0000\n",
            "L-BFGS | iteration: 523, total loss=1333999488.0000, content_loss=902483789.0625, style loss=408520986.3281, tv loss=22994736.0000\n",
            "L-BFGS | iteration: 524, total loss=1333756416.0000, content_loss=902494921.8750, style loss=408266074.2188, tv loss=22995442.0000\n",
            "L-BFGS | iteration: 525, total loss=1333432192.0000, content_loss=902406250.0000, style loss=408033720.7031, tv loss=22992254.0000\n",
            "L-BFGS | iteration: 526, total loss=1332840832.0000, content_loss=902138867.1875, style loss=407720859.3750, tv loss=22981084.0000\n",
            "L-BFGS | iteration: 527, total loss=1332478080.0000, content_loss=902130859.3750, style loss=407366953.1250, tv loss=22980170.0000\n",
            "L-BFGS | iteration: 528, total loss=1332017280.0000, content_loss=902031835.9375, style loss=407010703.1250, tv loss=22974796.0000\n",
            "L-BFGS | iteration: 529, total loss=1331431808.0000, content_loss=901784960.9375, style loss=406678798.8281, tv loss=22968004.0000\n",
            "L-BFGS | iteration: 530, total loss=1330823424.0000, content_loss=901569628.9062, style loss=406295390.6250, tv loss=22958512.0000\n",
            "L-BFGS | iteration: 531, total loss=1330411008.0000, content_loss=901538769.5312, style loss=405914384.7656, tv loss=22957848.0000\n",
            "L-BFGS | iteration: 532, total loss=1330138368.0000, content_loss=901461328.1250, style loss=405722548.8281, tv loss=22954438.0000\n",
            "L-BFGS | iteration: 533, total loss=1329787776.0000, content_loss=901499414.0625, style loss=405336005.8594, tv loss=22952290.0000\n",
            "L-BFGS | iteration: 534, total loss=1329307648.0000, content_loss=901231250.0000, style loss=405132802.7344, tv loss=22943572.0000\n",
            "L-BFGS | iteration: 535, total loss=1328792704.0000, content_loss=901165429.6875, style loss=404686875.0000, tv loss=22940360.0000\n",
            "L-BFGS | iteration: 536, total loss=1328275968.0000, content_loss=900994921.8750, style loss=404345625.0000, tv loss=22935460.0000\n",
            "L-BFGS | iteration: 537, total loss=1327742336.0000, content_loss=900598339.8438, style loss=404216572.2656, tv loss=22927440.0000\n",
            "L-BFGS | iteration: 538, total loss=1327227008.0000, content_loss=900609375.0000, style loss=403693886.7188, tv loss=22923722.0000\n",
            "L-BFGS | iteration: 539, total loss=1326881792.0000, content_loss=900463476.5625, style loss=403499267.5781, tv loss=22919084.0000\n",
            "L-BFGS | iteration: 540, total loss=1326360704.0000, content_loss=900263964.8438, style loss=403183330.0781, tv loss=22913446.0000\n",
            "L-BFGS | iteration: 541, total loss=1325982208.0000, content_loss=900085546.8750, style loss=402993017.5781, tv loss=22903496.0000\n",
            "L-BFGS | iteration: 542, total loss=1325527552.0000, content_loss=900139843.7500, style loss=402484980.4688, tv loss=22902782.0000\n",
            "L-BFGS | iteration: 543, total loss=1325121536.0000, content_loss=899955566.4062, style loss=402267011.7188, tv loss=22898992.0000\n",
            "L-BFGS | iteration: 544, total loss=1324746624.0000, content_loss=899736132.8125, style loss=402116015.6250, tv loss=22894524.0000\n",
            "L-BFGS | iteration: 545, total loss=1324211840.0000, content_loss=899360742.1875, style loss=401965517.5781, tv loss=22885564.0000\n",
            "L-BFGS | iteration: 546, total loss=1323768960.0000, content_loss=899147265.6250, style loss=401740605.4688, tv loss=22881148.0000\n",
            "L-BFGS | iteration: 547, total loss=1323488256.0000, content_loss=899025781.2500, style loss=401584042.9688, tv loss=22878528.0000\n",
            "L-BFGS | iteration: 548, total loss=1323082368.0000, content_loss=898802539.0625, style loss=401406152.3438, tv loss=22873728.0000\n",
            "L-BFGS | iteration: 549, total loss=1322639744.0000, content_loss=898708691.4062, style loss=401063496.0938, tv loss=22867632.0000\n",
            "L-BFGS | iteration: 550, total loss=1322160768.0000, content_loss=898446191.4062, style loss=400854345.7031, tv loss=22860156.0000\n",
            "L-BFGS | iteration: 551, total loss=1321744640.0000, content_loss=898530566.4062, style loss=400357119.1406, tv loss=22856956.0000\n",
            "L-BFGS | iteration: 552, total loss=1321480576.0000, content_loss=898422167.9688, style loss=400207177.7344, tv loss=22851178.0000\n",
            "L-BFGS | iteration: 553, total loss=1321160192.0000, content_loss=898476269.5312, style loss=399833818.3594, tv loss=22850076.0000\n",
            "L-BFGS | iteration: 554, total loss=1320802944.0000, content_loss=898239843.7500, style loss=399718798.8281, tv loss=22844272.0000\n",
            "L-BFGS | iteration: 555, total loss=1320420096.0000, content_loss=898104101.5625, style loss=399474814.4531, tv loss=22841092.0000\n",
            "L-BFGS | iteration: 556, total loss=1319838976.0000, content_loss=897799121.0938, style loss=399204931.6406, tv loss=22834940.0000\n",
            "L-BFGS | iteration: 557, total loss=1319365760.0000, content_loss=897676855.4688, style loss=398860546.8750, tv loss=22828376.0000\n",
            "L-BFGS | iteration: 558, total loss=1319004032.0000, content_loss=897637304.6875, style loss=398542031.2500, tv loss=22824700.0000\n",
            "L-BFGS | iteration: 559, total loss=1318680192.0000, content_loss=897558398.4375, style loss=398303261.7188, tv loss=22818554.0000\n",
            "L-BFGS | iteration: 560, total loss=1318328704.0000, content_loss=897595312.5000, style loss=397917568.3594, tv loss=22815844.0000\n",
            "L-BFGS | iteration: 561, total loss=1317935872.0000, content_loss=897405175.7812, style loss=397723037.1094, tv loss=22807634.0000\n",
            "L-BFGS | iteration: 562, total loss=1317532800.0000, content_loss=897300097.6562, style loss=397427783.2031, tv loss=22804858.0000\n",
            "L-BFGS | iteration: 563, total loss=1317071488.0000, content_loss=896988574.2188, style loss=397282968.7500, tv loss=22799952.0000\n",
            "L-BFGS | iteration: 564, total loss=1316626944.0000, content_loss=896849902.3438, style loss=396983027.3438, tv loss=22793956.0000\n",
            "L-BFGS | iteration: 565, total loss=1316272640.0000, content_loss=896648242.1875, style loss=396834375.0000, tv loss=22789952.0000\n",
            "L-BFGS | iteration: 566, total loss=1315939456.0000, content_loss=896535156.2500, style loss=396618105.4688, tv loss=22786176.0000\n",
            "L-BFGS | iteration: 567, total loss=1315442048.0000, content_loss=896391503.9062, style loss=396272343.7500, tv loss=22778272.0000\n",
            "L-BFGS | iteration: 568, total loss=1314877952.0000, content_loss=896331738.2812, style loss=395774765.6250, tv loss=22771416.0000\n",
            "L-BFGS | iteration: 569, total loss=1314562048.0000, content_loss=896212011.7188, style loss=395588261.7188, tv loss=22761772.0000\n",
            "L-BFGS | iteration: 570, total loss=1314294528.0000, content_loss=896388281.2500, style loss=395144677.7344, tv loss=22761658.0000\n",
            "L-BFGS | iteration: 571, total loss=1314026496.0000, content_loss=896213671.8750, style loss=395052890.6250, tv loss=22759904.0000\n",
            "L-BFGS | iteration: 572, total loss=1313750784.0000, content_loss=895998242.1875, style loss=394995673.8281, tv loss=22756862.0000\n",
            "L-BFGS | iteration: 573, total loss=1313372416.0000, content_loss=895777441.4062, style loss=394843037.1094, tv loss=22751828.0000\n",
            "L-BFGS | iteration: 574, total loss=1312878464.0000, content_loss=895528710.9375, style loss=394604121.0938, tv loss=22745638.0000\n",
            "L-BFGS | iteration: 575, total loss=1312466688.0000, content_loss=895573144.5312, style loss=394156083.9844, tv loss=22737384.0000\n",
            "L-BFGS | iteration: 576, total loss=1312106496.0000, content_loss=895621679.6875, style loss=393749296.8750, tv loss=22735468.0000\n",
            "L-BFGS | iteration: 577, total loss=1311805056.0000, content_loss=895559375.0000, style loss=393517089.8438, tv loss=22728566.0000\n",
            "L-BFGS | iteration: 578, total loss=1311482880.0000, content_loss=895609277.3438, style loss=393147714.8438, tv loss=22725832.0000\n",
            "L-BFGS | iteration: 579, total loss=1311149696.0000, content_loss=895482421.8750, style loss=392947324.2188, tv loss=22719888.0000\n",
            "L-BFGS | iteration: 580, total loss=1310640256.0000, content_loss=895244531.2500, style loss=392683886.7188, tv loss=22711804.0000\n",
            "L-BFGS | iteration: 581, total loss=1310338176.0000, content_loss=895097167.9688, style loss=392536376.9531, tv loss=22704636.0000\n",
            "L-BFGS | iteration: 582, total loss=1310081792.0000, content_loss=895021679.6875, style loss=392355996.0938, tv loss=22704184.0000\n",
            "L-BFGS | iteration: 583, total loss=1309722240.0000, content_loss=894803613.2812, style loss=392220820.3125, tv loss=22697830.0000\n",
            "L-BFGS | iteration: 584, total loss=1309467264.0000, content_loss=894718750.0000, style loss=392051689.4531, tv loss=22696786.0000\n",
            "L-BFGS | iteration: 585, total loss=1309024768.0000, content_loss=894513964.8438, style loss=391820917.9688, tv loss=22689848.0000\n",
            "L-BFGS | iteration: 586, total loss=1308547200.0000, content_loss=894378710.9375, style loss=391485937.5000, tv loss=22682516.0000\n",
            "L-BFGS | iteration: 587, total loss=1308198400.0000, content_loss=894341894.5312, style loss=391179873.0469, tv loss=22676664.0000\n",
            "L-BFGS | iteration: 588, total loss=1307942784.0000, content_loss=894255371.0938, style loss=391013525.3906, tv loss=22673888.0000\n",
            "L-BFGS | iteration: 589, total loss=1307632768.0000, content_loss=894206347.6562, style loss=390755332.0312, tv loss=22671068.0000\n",
            "L-BFGS | iteration: 590, total loss=1307365120.0000, content_loss=894075195.3125, style loss=390624521.4844, tv loss=22665430.0000\n",
            "L-BFGS | iteration: 591, total loss=1306936704.0000, content_loss=893986718.7500, style loss=390289921.8750, tv loss=22660142.0000\n",
            "L-BFGS | iteration: 592, total loss=1306510720.0000, content_loss=893787304.6875, style loss=390070839.8438, tv loss=22652528.0000\n",
            "L-BFGS | iteration: 593, total loss=1306034560.0000, content_loss=893660351.5625, style loss=389726865.2344, tv loss=22647296.0000\n",
            "L-BFGS | iteration: 594, total loss=1305732480.0000, content_loss=893569824.2188, style loss=389518593.7500, tv loss=22644048.0000\n",
            "L-BFGS | iteration: 595, total loss=1305424128.0000, content_loss=893370214.8438, style loss=389415908.2031, tv loss=22637984.0000\n",
            "L-BFGS | iteration: 596, total loss=1305049472.0000, content_loss=893279492.1875, style loss=389136562.5000, tv loss=22633294.0000\n",
            "L-BFGS | iteration: 597, total loss=1304685440.0000, content_loss=893089355.4688, style loss=388971533.2031, tv loss=22624614.0000\n",
            "L-BFGS | iteration: 598, total loss=1304305920.0000, content_loss=893089160.1562, style loss=388594394.5312, tv loss=22622396.0000\n",
            "L-BFGS | iteration: 599, total loss=1303972608.0000, content_loss=893088867.1875, style loss=388266123.0469, tv loss=22617594.0000\n",
            "L-BFGS | iteration: 600, total loss=1303576320.0000, content_loss=892932324.2188, style loss=388033330.0781, tv loss=22610706.0000\n",
            "L-BFGS | iteration: 601, total loss=1303281024.0000, content_loss=893064648.4375, style loss=387608994.1406, tv loss=22607400.0000\n",
            "L-BFGS | iteration: 602, total loss=1303038208.0000, content_loss=892934179.6875, style loss=387501562.5000, tv loss=22602560.0000\n",
            "L-BFGS | iteration: 603, total loss=1302647424.0000, content_loss=892677343.7500, style loss=387373681.6406, tv loss=22596430.0000\n",
            "L-BFGS | iteration: 604, total loss=1302379520.0000, content_loss=892525781.2500, style loss=387263085.9375, tv loss=22590728.0000\n",
            "L-BFGS | iteration: 605, total loss=1302005248.0000, content_loss=892463671.8750, style loss=386953828.1250, tv loss=22587814.0000\n",
            "L-BFGS | iteration: 606, total loss=1301671040.0000, content_loss=892362402.3438, style loss=386725869.1406, tv loss=22582812.0000\n",
            "L-BFGS | iteration: 607, total loss=1301353984.0000, content_loss=892381933.5938, style loss=386392587.8906, tv loss=22579520.0000\n",
            "L-BFGS | iteration: 608, total loss=1301016192.0000, content_loss=892314746.0938, style loss=386128916.0156, tv loss=22572544.0000\n",
            "L-BFGS | iteration: 609, total loss=1300619136.0000, content_loss=892275488.2812, style loss=385775859.3750, tv loss=22567840.0000\n",
            "L-BFGS | iteration: 610, total loss=1300294016.0000, content_loss=891988964.8438, style loss=385745449.2188, tv loss=22559624.0000\n",
            "L-BFGS | iteration: 611, total loss=1300022912.0000, content_loss=891864160.1562, style loss=385599345.7031, tv loss=22559422.0000\n",
            "L-BFGS | iteration: 612, total loss=1299781632.0000, content_loss=891653320.3125, style loss=385572861.3281, tv loss=22555452.0000\n",
            "L-BFGS | iteration: 613, total loss=1299430784.0000, content_loss=891385742.1875, style loss=385495839.8438, tv loss=22549252.0000\n",
            "L-BFGS | iteration: 614, total loss=1299087488.0000, content_loss=891242382.8125, style loss=385301425.7812, tv loss=22543738.0000\n",
            "L-BFGS | iteration: 615, total loss=1298765056.0000, content_loss=891207421.8750, style loss=385018769.5312, tv loss=22538832.0000\n",
            "L-BFGS | iteration: 616, total loss=1298465408.0000, content_loss=891180273.4375, style loss=384751582.0312, tv loss=22533488.0000\n",
            "L-BFGS | iteration: 617, total loss=1298070144.0000, content_loss=891073242.1875, style loss=384470537.1094, tv loss=22526316.0000\n",
            "L-BFGS | iteration: 618, total loss=1297677952.0000, content_loss=891015820.3125, style loss=384144902.3438, tv loss=22517226.0000\n",
            "L-BFGS | iteration: 619, total loss=1297377792.0000, content_loss=891026660.1562, style loss=383835087.8906, tv loss=22515996.0000\n",
            "L-BFGS | iteration: 620, total loss=1297155072.0000, content_loss=890789257.8125, style loss=383857294.9219, tv loss=22508528.0000\n",
            "L-BFGS | iteration: 621, total loss=1296925184.0000, content_loss=890826464.8438, style loss=383589931.6406, tv loss=22508840.0000\n",
            "L-BFGS | iteration: 622, total loss=1296655232.0000, content_loss=890789550.7812, style loss=383359248.0469, tv loss=22506426.0000\n",
            "L-BFGS | iteration: 623, total loss=1296244736.0000, content_loss=890641601.5625, style loss=383103486.3281, tv loss=22499588.0000\n",
            "L-BFGS | iteration: 624, total loss=1295849984.0000, content_loss=890448144.5312, style loss=382910009.7656, tv loss=22491940.0000\n",
            "L-BFGS | iteration: 625, total loss=1295506048.0000, content_loss=890321386.7188, style loss=382697285.1562, tv loss=22487394.0000\n",
            "L-BFGS | iteration: 626, total loss=1295262592.0000, content_loss=890232519.5312, style loss=382546083.9844, tv loss=22483936.0000\n",
            "L-BFGS | iteration: 627, total loss=1294934784.0000, content_loss=890053417.9688, style loss=382404814.4531, tv loss=22476526.0000\n",
            "L-BFGS | iteration: 628, total loss=1294647552.0000, content_loss=889985058.5938, style loss=382191123.0469, tv loss=22471254.0000\n",
            "L-BFGS | iteration: 629, total loss=1294409856.0000, content_loss=889941015.6250, style loss=382000048.8281, tv loss=22468736.0000\n",
            "L-BFGS | iteration: 630, total loss=1294063872.0000, content_loss=889861621.0938, style loss=381738574.2188, tv loss=22463764.0000\n",
            "L-BFGS | iteration: 631, total loss=1293788416.0000, content_loss=889805761.7188, style loss=381523271.4844, tv loss=22459336.0000\n",
            "L-BFGS | iteration: 632, total loss=1293341184.0000, content_loss=889650097.6562, style loss=381241289.0625, tv loss=22449780.0000\n",
            "L-BFGS | iteration: 633, total loss=1293080960.0000, content_loss=889677832.0312, style loss=380957080.0781, tv loss=22446048.0000\n",
            "L-BFGS | iteration: 634, total loss=1292907648.0000, content_loss=889467480.4688, style loss=381001787.1094, tv loss=22438368.0000\n",
            "L-BFGS | iteration: 635, total loss=1292667008.0000, content_loss=889429980.4688, style loss=380796884.7656, tv loss=22440200.0000\n",
            "L-BFGS | iteration: 636, total loss=1292491264.0000, content_loss=889372949.2188, style loss=380679931.6406, tv loss=22438336.0000\n",
            "L-BFGS | iteration: 637, total loss=1292177408.0000, content_loss=889278320.3125, style loss=380466796.8750, tv loss=22432268.0000\n",
            "L-BFGS | iteration: 638, total loss=1291841536.0000, content_loss=889187792.9688, style loss=380228232.4219, tv loss=22425618.0000\n",
            "L-BFGS | iteration: 639, total loss=1291461504.0000, content_loss=889194824.2188, style loss=379848046.8750, tv loss=22418698.0000\n",
            "L-BFGS | iteration: 640, total loss=1291191808.0000, content_loss=889115527.3438, style loss=379660869.1406, tv loss=22415354.0000\n",
            "L-BFGS | iteration: 641, total loss=1290937728.0000, content_loss=889000488.2812, style loss=379526308.5938, tv loss=22410896.0000\n",
            "L-BFGS | iteration: 642, total loss=1290592640.0000, content_loss=888927734.3750, style loss=379260087.8906, tv loss=22404772.0000\n",
            "L-BFGS | iteration: 643, total loss=1290306944.0000, content_loss=888602929.6875, style loss=379305820.3125, tv loss=22398164.0000\n",
            "L-BFGS | iteration: 644, total loss=1290066688.0000, content_loss=888579101.5625, style loss=379090634.7656, tv loss=22396868.0000\n",
            "L-BFGS | iteration: 645, total loss=1289819392.0000, content_loss=888416210.9375, style loss=379012705.0781, tv loss=22390498.0000\n",
            "L-BFGS | iteration: 646, total loss=1289594496.0000, content_loss=888432714.8438, style loss=378771884.7656, tv loss=22389846.0000\n",
            "L-BFGS | iteration: 647, total loss=1289301248.0000, content_loss=888326562.5000, style loss=378591416.0156, tv loss=22383184.0000\n",
            "L-BFGS | iteration: 648, total loss=1288997504.0000, content_loss=888130468.7500, style loss=378492480.4688, tv loss=22374544.0000\n",
            "L-BFGS | iteration: 649, total loss=1288765312.0000, content_loss=888070410.1562, style loss=378322089.8438, tv loss=22372828.0000\n",
            "L-BFGS | iteration: 650, total loss=1288362112.0000, content_loss=887826074.2188, style loss=378169599.6094, tv loss=22366468.0000\n",
            "L-BFGS | iteration: 651, total loss=1287987968.0000, content_loss=887660644.5312, style loss=377969296.8750, tv loss=22357978.0000\n",
            "L-BFGS | iteration: 652, total loss=1287705088.0000, content_loss=887507324.2188, style loss=377843642.5781, tv loss=22354232.0000\n",
            "L-BFGS | iteration: 653, total loss=1287524736.0000, content_loss=887497265.6250, style loss=377674511.7188, tv loss=22353048.0000\n",
            "L-BFGS | iteration: 654, total loss=1287204480.0000, content_loss=887438867.1875, style loss=377419951.1719, tv loss=22345780.0000\n",
            "L-BFGS | iteration: 655, total loss=1286928896.0000, content_loss=887522167.9688, style loss=377063173.8281, tv loss=22343606.0000\n",
            "L-BFGS | iteration: 656, total loss=1286628352.0000, content_loss=887325195.3125, style loss=376968222.6562, tv loss=22334936.0000\n",
            "L-BFGS | iteration: 657, total loss=1286354560.0000, content_loss=887174414.0625, style loss=376849658.2031, tv loss=22330496.0000\n",
            "L-BFGS | iteration: 658, total loss=1286026496.0000, content_loss=886870800.7812, style loss=376831611.3281, tv loss=22324080.0000\n",
            "L-BFGS | iteration: 659, total loss=1285689984.0000, content_loss=886682910.1562, style loss=376689169.9219, tv loss=22317920.0000\n",
            "L-BFGS | iteration: 660, total loss=1285431296.0000, content_loss=886538281.2500, style loss=376578720.7031, tv loss=22314224.0000\n",
            "L-BFGS | iteration: 661, total loss=1285179904.0000, content_loss=886358984.3750, style loss=376510634.7656, tv loss=22310292.0000\n",
            "L-BFGS | iteration: 662, total loss=1284932992.0000, content_loss=886264941.4062, style loss=376362568.3594, tv loss=22305404.0000\n",
            "L-BFGS | iteration: 663, total loss=1284634752.0000, content_loss=886233496.0938, style loss=376099511.7188, tv loss=22301852.0000\n",
            "L-BFGS | iteration: 664, total loss=1284384384.0000, content_loss=886039941.4062, style loss=376052841.7969, tv loss=22291582.0000\n",
            "L-BFGS | iteration: 665, total loss=1284125440.0000, content_loss=886163281.2500, style loss=375670400.3906, tv loss=22291704.0000\n",
            "L-BFGS | iteration: 666, total loss=1283908480.0000, content_loss=886032519.5312, style loss=375587666.0156, tv loss=22288264.0000\n",
            "L-BFGS | iteration: 667, total loss=1283644288.0000, content_loss=885847753.9062, style loss=375513808.5938, tv loss=22282774.0000\n",
            "L-BFGS | iteration: 668, total loss=1283333504.0000, content_loss=885676953.1250, style loss=375380332.0312, tv loss=22276254.0000\n",
            "L-BFGS | iteration: 669, total loss=1283063424.0000, content_loss=885566796.8750, style loss=375227783.2031, tv loss=22268808.0000\n",
            "L-BFGS | iteration: 670, total loss=1282796672.0000, content_loss=885566796.8750, style loss=374961972.6562, tv loss=22267936.0000\n",
            "L-BFGS | iteration: 671, total loss=1282600576.0000, content_loss=885544531.2500, style loss=374791757.8125, tv loss=22264350.0000\n",
            "L-BFGS | iteration: 672, total loss=1282316928.0000, content_loss=885552343.7500, style loss=374503271.4844, tv loss=22261340.0000\n",
            "L-BFGS | iteration: 673, total loss=1281999104.0000, content_loss=885366503.9062, style loss=374381337.8906, tv loss=22251202.0000\n",
            "L-BFGS | iteration: 674, total loss=1281731584.0000, content_loss=885349804.6875, style loss=374131845.7031, tv loss=22250032.0000\n",
            "L-BFGS | iteration: 675, total loss=1281489152.0000, content_loss=885172460.9375, style loss=374071318.3594, tv loss=22245430.0000\n",
            "L-BFGS | iteration: 676, total loss=1281201536.0000, content_loss=885050292.9688, style loss=373911855.4688, tv loss=22239420.0000\n",
            "L-BFGS | iteration: 677, total loss=1280932480.0000, content_loss=884963183.5938, style loss=373735283.2031, tv loss=22234044.0000\n",
            "L-BFGS | iteration: 678, total loss=1280668160.0000, content_loss=884940820.3125, style loss=373497421.8750, tv loss=22230020.0000\n",
            "L-BFGS | iteration: 679, total loss=1280314240.0000, content_loss=884828027.3438, style loss=373264658.2031, tv loss=22221564.0000\n",
            "L-BFGS | iteration: 680, total loss=1280009472.0000, content_loss=884773242.1875, style loss=373019179.6875, tv loss=22217044.0000\n",
            "L-BFGS | iteration: 681, total loss=1279768576.0000, content_loss=884620703.1250, style loss=372937324.2188, tv loss=22210584.0000\n",
            "L-BFGS | iteration: 682, total loss=1279546368.0000, content_loss=884511425.7812, style loss=372827050.7812, tv loss=22207892.0000\n",
            "L-BFGS | iteration: 683, total loss=1279357568.0000, content_loss=884334863.2812, style loss=372822480.4688, tv loss=22200224.0000\n",
            "L-BFGS | iteration: 684, total loss=1279111808.0000, content_loss=884311035.1562, style loss=372601171.8750, tv loss=22199590.0000\n",
            "L-BFGS | iteration: 685, total loss=1278801024.0000, content_loss=884111230.4688, style loss=372494589.8438, tv loss=22195220.0000\n",
            "L-BFGS | iteration: 686, total loss=1278537984.0000, content_loss=884080273.4375, style loss=372267392.5781, tv loss=22190344.0000\n",
            "L-BFGS | iteration: 687, total loss=1278283136.0000, content_loss=883891503.9062, style loss=372206337.8906, tv loss=22185356.0000\n",
            "L-BFGS | iteration: 688, total loss=1278019200.0000, content_loss=883722656.2500, style loss=372116572.2656, tv loss=22180028.0000\n",
            "L-BFGS | iteration: 689, total loss=1277732992.0000, content_loss=883648242.1875, style loss=371914335.9375, tv loss=22170310.0000\n",
            "L-BFGS | iteration: 690, total loss=1277484800.0000, content_loss=883547460.9375, style loss=371771484.3750, tv loss=22165868.0000\n",
            "L-BFGS | iteration: 691, total loss=1277298432.0000, content_loss=883592773.4375, style loss=371539980.4688, tv loss=22165732.0000\n",
            "L-BFGS | iteration: 692, total loss=1277086208.0000, content_loss=883520507.8125, style loss=371407822.2656, tv loss=22157800.0000\n",
            "L-BFGS | iteration: 693, total loss=1276907008.0000, content_loss=883625585.9375, style loss=371122939.4531, tv loss=22158400.0000\n",
            "L-BFGS | iteration: 694, total loss=1276702720.0000, content_loss=883597851.5625, style loss=370949531.2500, tv loss=22155428.0000\n",
            "L-BFGS | iteration: 695, total loss=1276327168.0000, content_loss=883380566.4062, style loss=370802636.7188, tv loss=22143988.0000\n",
            "L-BFGS | iteration: 696, total loss=1276093568.0000, content_loss=883395117.1875, style loss=370554814.4531, tv loss=22143582.0000\n",
            "L-BFGS | iteration: 697, total loss=1275865856.0000, content_loss=883264062.5000, style loss=370462851.5625, tv loss=22138852.0000\n",
            "L-BFGS | iteration: 698, total loss=1275528704.0000, content_loss=882997656.2500, style loss=370402470.7031, tv loss=22128636.0000\n",
            "L-BFGS | iteration: 699, total loss=1275276416.0000, content_loss=882790234.3750, style loss=370363183.5938, tv loss=22123052.0000\n",
            "L-BFGS | iteration: 700, total loss=1275021440.0000, content_loss=882749023.4375, style loss=370154970.7031, tv loss=22117508.0000\n",
            "L-BFGS | iteration: 701, total loss=1274766592.0000, content_loss=882783300.7812, style loss=369868330.0781, tv loss=22114960.0000\n",
            "L-BFGS | iteration: 702, total loss=1274571776.0000, content_loss=882717675.7812, style loss=369746601.5625, tv loss=22107464.0000\n",
            "L-BFGS | iteration: 703, total loss=1274343296.0000, content_loss=882713476.5625, style loss=369523212.8906, tv loss=22106666.0000\n",
            "L-BFGS | iteration: 704, total loss=1274069120.0000, content_loss=882678417.9688, style loss=369288251.9531, tv loss=22102420.0000\n",
            "L-BFGS | iteration: 705, total loss=1273762688.0000, content_loss=882488964.8438, style loss=369179033.2031, tv loss=22094680.0000\n",
            "L-BFGS | iteration: 706, total loss=1273544064.0000, content_loss=882438867.1875, style loss=369013564.4531, tv loss=22091616.0000\n",
            "L-BFGS | iteration: 707, total loss=1273394560.0000, content_loss=882325781.2500, style loss=368980781.2500, tv loss=22088116.0000\n",
            "L-BFGS | iteration: 708, total loss=1273107840.0000, content_loss=882074902.3438, style loss=368951777.3438, tv loss=22081116.0000\n",
            "L-BFGS | iteration: 709, total loss=1272794624.0000, content_loss=882033007.8125, style loss=368687226.5625, tv loss=22074364.0000\n",
            "L-BFGS | iteration: 710, total loss=1272570752.0000, content_loss=881882324.2188, style loss=368620634.7656, tv loss=22067898.0000\n",
            "L-BFGS | iteration: 711, total loss=1272386944.0000, content_loss=881973925.7812, style loss=368343486.3281, tv loss=22069468.0000\n",
            "L-BFGS | iteration: 712, total loss=1272206464.0000, content_loss=881923046.8750, style loss=368219472.6562, tv loss=22063908.0000\n",
            "L-BFGS | iteration: 713, total loss=1272038912.0000, content_loss=881876660.1562, style loss=368100468.7500, tv loss=22061832.0000\n",
            "L-BFGS | iteration: 714, total loss=1271709952.0000, content_loss=881695996.0938, style loss=367960283.2031, tv loss=22053696.0000\n",
            "L-BFGS | iteration: 715, total loss=1271433088.0000, content_loss=881634082.0312, style loss=367749814.4531, tv loss=22049128.0000\n",
            "L-BFGS | iteration: 716, total loss=1271155072.0000, content_loss=881521679.6875, style loss=367591787.1094, tv loss=22041660.0000\n",
            "L-BFGS | iteration: 717, total loss=1270925312.0000, content_loss=881421777.3438, style loss=367465693.3594, tv loss=22037850.0000\n",
            "L-BFGS | iteration: 718, total loss=1270734336.0000, content_loss=881333398.4375, style loss=367367548.8281, tv loss=22033348.0000\n",
            "L-BFGS | iteration: 719, total loss=1270444800.0000, content_loss=881213281.2500, style loss=367203837.8906, tv loss=22027604.0000\n",
            "L-BFGS | iteration: 720, total loss=1270182912.0000, content_loss=881059667.9688, style loss=367104843.7500, tv loss=22018432.0000\n",
            "L-BFGS | iteration: 721, total loss=1269968128.0000, content_loss=881060449.2188, style loss=366888984.3750, tv loss=22018666.0000\n",
            "L-BFGS | iteration: 722, total loss=1269834880.0000, content_loss=880996777.3438, style loss=366824150.3906, tv loss=22013904.0000\n",
            "L-BFGS | iteration: 723, total loss=1269653504.0000, content_loss=881046289.0625, style loss=366595927.7344, tv loss=22011212.0000\n",
            "L-BFGS | iteration: 724, total loss=1269456384.0000, content_loss=880985058.5938, style loss=366464707.0312, tv loss=22006504.0000\n",
            "L-BFGS | iteration: 725, total loss=1269159552.0000, content_loss=880930273.4375, style loss=366230009.7656, tv loss=21999230.0000\n",
            "L-BFGS | iteration: 726, total loss=1268881408.0000, content_loss=880759375.0000, style loss=366127236.3281, tv loss=21994762.0000\n",
            "L-BFGS | iteration: 727, total loss=1268563200.0000, content_loss=880575000.0000, style loss=366002255.8594, tv loss=21985990.0000\n",
            "L-BFGS | iteration: 728, total loss=1268313088.0000, content_loss=880387500.0000, style loss=365943515.6250, tv loss=21982030.0000\n",
            "L-BFGS | iteration: 729, total loss=1268106368.0000, content_loss=880284960.9375, style loss=365843525.3906, tv loss=21977858.0000\n",
            "L-BFGS | iteration: 730, total loss=1267871872.0000, content_loss=880227539.0625, style loss=365671992.1875, tv loss=21972360.0000\n",
            "L-BFGS | iteration: 731, total loss=1267660288.0000, content_loss=880139648.4375, style loss=365552724.6094, tv loss=21967836.0000\n",
            "L-BFGS | iteration: 732, total loss=1267452928.0000, content_loss=880129296.8750, style loss=365359277.3438, tv loss=21964420.0000\n",
            "L-BFGS | iteration: 733, total loss=1267226752.0000, content_loss=880041796.8750, style loss=365226298.8281, tv loss=21958616.0000\n",
            "L-BFGS | iteration: 734, total loss=1267011200.0000, content_loss=880045214.8438, style loss=365009531.2500, tv loss=21956472.0000\n",
            "L-BFGS | iteration: 735, total loss=1266804096.0000, content_loss=879818457.0312, style loss=365038330.0781, tv loss=21947302.0000\n",
            "L-BFGS | iteration: 736, total loss=1266596736.0000, content_loss=879749902.3438, style loss=364900371.0938, tv loss=21946464.0000\n",
            "L-BFGS | iteration: 737, total loss=1266408320.0000, content_loss=879628222.6562, style loss=364837353.5156, tv loss=21942810.0000\n",
            "L-BFGS | iteration: 738, total loss=1266178432.0000, content_loss=879565820.3125, style loss=364674931.6406, tv loss=21937712.0000\n",
            "L-BFGS | iteration: 739, total loss=1265903232.0000, content_loss=879442773.4375, style loss=364530498.0469, tv loss=21929944.0000\n",
            "L-BFGS | iteration: 740, total loss=1265687552.0000, content_loss=879499609.3750, style loss=364261523.4375, tv loss=21926372.0000\n",
            "L-BFGS | iteration: 741, total loss=1265496320.0000, content_loss=879445507.8125, style loss=364128251.9531, tv loss=21922538.0000\n",
            "L-BFGS | iteration: 742, total loss=1265298560.0000, content_loss=879361816.4062, style loss=364018798.8281, tv loss=21917934.0000\n",
            "L-BFGS | iteration: 743, total loss=1265066624.0000, content_loss=879305859.3750, style loss=363847558.5938, tv loss=21913252.0000\n",
            "L-BFGS | iteration: 744, total loss=1264810880.0000, content_loss=879093750.0000, style loss=363812548.8281, tv loss=21904530.0000\n",
            "L-BFGS | iteration: 745, total loss=1264619520.0000, content_loss=879149804.6875, style loss=363564990.2344, tv loss=21904598.0000\n",
            "L-BFGS | iteration: 746, total loss=1264468736.0000, content_loss=879092578.1250, style loss=363477509.7656, tv loss=21898650.0000\n",
            "L-BFGS | iteration: 747, total loss=1264313088.0000, content_loss=878977539.0625, style loss=363439482.4219, tv loss=21896100.0000\n",
            "L-BFGS | iteration: 748, total loss=1264102144.0000, content_loss=878879882.8125, style loss=363330292.9688, tv loss=21891906.0000\n",
            "L-BFGS | iteration: 749, total loss=1263824256.0000, content_loss=878651171.8750, style loss=363288867.1875, tv loss=21884098.0000\n",
            "L-BFGS | iteration: 750, total loss=1263618944.0000, content_loss=878709472.6562, style loss=363030585.9375, tv loss=21878960.0000\n",
            "L-BFGS | iteration: 751, total loss=1263466112.0000, content_loss=878684375.0000, style loss=362903583.9844, tv loss=21878088.0000\n",
            "L-BFGS | iteration: 752, total loss=1263240704.0000, content_loss=878541113.2812, style loss=362828437.5000, tv loss=21871078.0000\n",
            "L-BFGS | iteration: 753, total loss=1263061248.0000, content_loss=878537304.6875, style loss=362653974.6094, tv loss=21869948.0000\n",
            "L-BFGS | iteration: 754, total loss=1262823680.0000, content_loss=878405078.1250, style loss=362556328.1250, tv loss=21862256.0000\n",
            "L-BFGS | iteration: 755, total loss=1262567552.0000, content_loss=878296972.6562, style loss=362414619.1406, tv loss=21855950.0000\n",
            "L-BFGS | iteration: 756, total loss=1262365440.0000, content_loss=878140820.3125, style loss=362373720.7031, tv loss=21850900.0000\n",
            "L-BFGS | iteration: 757, total loss=1262159616.0000, content_loss=878124707.0312, style loss=362186953.1250, tv loss=21847884.0000\n",
            "L-BFGS | iteration: 758, total loss=1262001664.0000, content_loss=878087695.3125, style loss=362069003.9062, tv loss=21844974.0000\n",
            "L-BFGS | iteration: 759, total loss=1261797120.0000, content_loss=878010839.8438, style loss=361945751.9531, tv loss=21840452.0000\n",
            "L-BFGS | iteration: 760, total loss=1261496320.0000, content_loss=877916699.2188, style loss=361746855.4688, tv loss=21832894.0000\n",
            "L-BFGS | iteration: 761, total loss=1261206912.0000, content_loss=877825683.5938, style loss=361556162.1094, tv loss=21825188.0000\n",
            "L-BFGS | iteration: 762, total loss=1260998272.0000, content_loss=877721484.3750, style loss=361456464.8438, tv loss=21820234.0000\n",
            "L-BFGS | iteration: 763, total loss=1260843648.0000, content_loss=877713183.5938, style loss=361311884.7656, tv loss=21818652.0000\n",
            "L-BFGS | iteration: 764, total loss=1260700672.0000, content_loss=877679101.5625, style loss=361206064.4531, tv loss=21815566.0000\n",
            "L-BFGS | iteration: 765, total loss=1260505728.0000, content_loss=877649121.0938, style loss=361044345.7031, tv loss=21812280.0000\n",
            "L-BFGS | iteration: 766, total loss=1260325376.0000, content_loss=877495898.4375, style loss=361029580.0781, tv loss=21799932.0000\n",
            "L-BFGS | iteration: 767, total loss=1260159232.0000, content_loss=877578515.6250, style loss=360777128.9062, tv loss=21803642.0000\n",
            "L-BFGS | iteration: 768, total loss=1260009728.0000, content_loss=877494335.9375, style loss=360713085.9375, tv loss=21802180.0000\n",
            "L-BFGS | iteration: 769, total loss=1259851008.0000, content_loss=877380175.7812, style loss=360673886.7188, tv loss=21796928.0000\n",
            "L-BFGS | iteration: 770, total loss=1259673088.0000, content_loss=877310546.8750, style loss=360569765.6250, tv loss=21792764.0000\n",
            "L-BFGS | iteration: 771, total loss=1259369600.0000, content_loss=877247558.5938, style loss=360339697.2656, tv loss=21782440.0000\n",
            "L-BFGS | iteration: 772, total loss=1259151616.0000, content_loss=877257226.5625, style loss=360116718.7500, tv loss=21777604.0000\n",
            "L-BFGS | iteration: 773, total loss=1258968960.0000, content_loss=877231347.6562, style loss=359961298.8281, tv loss=21776268.0000\n",
            "L-BFGS | iteration: 774, total loss=1258817664.0000, content_loss=877118359.3750, style loss=359928837.8906, tv loss=21770528.0000\n",
            "L-BFGS | iteration: 775, total loss=1258666880.0000, content_loss=877122460.9375, style loss=359775029.2969, tv loss=21769304.0000\n",
            "L-BFGS | iteration: 776, total loss=1258469376.0000, content_loss=877051171.8750, style loss=359654941.4062, tv loss=21763200.0000\n",
            "L-BFGS | iteration: 777, total loss=1258258560.0000, content_loss=876962597.6562, style loss=359537167.9688, tv loss=21758844.0000\n",
            "L-BFGS | iteration: 778, total loss=1258015232.0000, content_loss=876854003.9062, style loss=359408876.9531, tv loss=21752292.0000\n",
            "L-BFGS | iteration: 779, total loss=1257787520.0000, content_loss=876736132.8125, style loss=359304843.7500, tv loss=21746516.0000\n",
            "L-BFGS | iteration: 780, total loss=1257617792.0000, content_loss=876715722.6562, style loss=359158271.4844, tv loss=21743714.0000\n",
            "L-BFGS | iteration: 781, total loss=1257474688.0000, content_loss=876663183.5938, style loss=359071611.3281, tv loss=21739942.0000\n",
            "L-BFGS | iteration: 782, total loss=1257299584.0000, content_loss=876597167.9688, style loss=358966171.8750, tv loss=21736216.0000\n",
            "L-BFGS | iteration: 783, total loss=1257140992.0000, content_loss=876545117.1875, style loss=358864951.1719, tv loss=21730966.0000\n",
            "L-BFGS | iteration: 784, total loss=1257004416.0000, content_loss=876535351.5625, style loss=358738037.1094, tv loss=21731006.0000\n",
            "L-BFGS | iteration: 785, total loss=1256830592.0000, content_loss=876359277.3438, style loss=358747646.4844, tv loss=21723684.0000\n",
            "L-BFGS | iteration: 786, total loss=1256720896.0000, content_loss=876399121.0938, style loss=358596738.2812, tv loss=21725078.0000\n",
            "L-BFGS | iteration: 787, total loss=1256571904.0000, content_loss=876383886.7188, style loss=358465166.0156, tv loss=21722864.0000\n",
            "L-BFGS | iteration: 788, total loss=1256379392.0000, content_loss=876305078.1250, style loss=358357587.8906, tv loss=21716714.0000\n",
            "L-BFGS | iteration: 789, total loss=1256139520.0000, content_loss=876197949.2188, style loss=358233369.1406, tv loss=21708102.0000\n",
            "L-BFGS | iteration: 790, total loss=1255898880.0000, content_loss=876087792.9688, style loss=358108095.7031, tv loss=21702928.0000\n",
            "L-BFGS | iteration: 791, total loss=1255691136.0000, content_loss=876003320.3125, style loss=357990761.7188, tv loss=21697064.0000\n",
            "L-BFGS | iteration: 792, total loss=1255530112.0000, content_loss=875910742.1875, style loss=357925166.0156, tv loss=21694248.0000\n",
            "L-BFGS | iteration: 793, total loss=1255342336.0000, content_loss=875862890.6250, style loss=357788994.1406, tv loss=21690328.0000\n",
            "L-BFGS | iteration: 794, total loss=1255148672.0000, content_loss=875752636.7188, style loss=357711064.4531, tv loss=21685036.0000\n",
            "L-BFGS | iteration: 795, total loss=1254959360.0000, content_loss=875642578.1250, style loss=357634892.5781, tv loss=21681912.0000\n",
            "L-BFGS | iteration: 796, total loss=1254783104.0000, content_loss=875601464.8438, style loss=357504433.5938, tv loss=21677244.0000\n",
            "L-BFGS | iteration: 797, total loss=1254588416.0000, content_loss=875513867.1875, style loss=357401132.8125, tv loss=21673444.0000\n",
            "L-BFGS | iteration: 798, total loss=1254441856.0000, content_loss=875439550.7812, style loss=357337705.0781, tv loss=21664664.0000\n",
            "L-BFGS | iteration: 799, total loss=1254275200.0000, content_loss=875542382.8125, style loss=357067705.0781, tv loss=21665212.0000\n",
            "L-BFGS | iteration: 800, total loss=1254123904.0000, content_loss=875425683.5938, style loss=357035771.4844, tv loss=21662484.0000\n",
            "L-BFGS | iteration: 801, total loss=1253966976.0000, content_loss=875326367.1875, style loss=356982802.7344, tv loss=21657878.0000\n",
            "L-BFGS | iteration: 802, total loss=1253749888.0000, content_loss=875194238.2812, style loss=356904404.2969, tv loss=21651260.0000\n",
            "L-BFGS | iteration: 803, total loss=1253529600.0000, content_loss=875087207.0312, style loss=356798144.5312, tv loss=21644262.0000\n",
            "L-BFGS | iteration: 804, total loss=1253349120.0000, content_loss=875078320.3125, style loss=356628398.4375, tv loss=21642420.0000\n",
            "L-BFGS | iteration: 805, total loss=1253218304.0000, content_loss=875043750.0000, style loss=356535878.9062, tv loss=21638712.0000\n",
            "L-BFGS | iteration: 806, total loss=1253005440.0000, content_loss=875043652.3438, style loss=356328046.8750, tv loss=21633828.0000\n",
            "L-BFGS | iteration: 807, total loss=1252806400.0000, content_loss=874918945.3125, style loss=356260751.9531, tv loss=21626650.0000\n",
            "L-BFGS | iteration: 808, total loss=1252607616.0000, content_loss=874903808.5938, style loss=356078730.4688, tv loss=21625098.0000\n",
            "L-BFGS | iteration: 809, total loss=1252427904.0000, content_loss=874824804.6875, style loss=355983105.4688, tv loss=21619906.0000\n",
            "L-BFGS | iteration: 810, total loss=1252237824.0000, content_loss=874746093.7500, style loss=355875000.0000, tv loss=21616618.0000\n",
            "L-BFGS | iteration: 811, total loss=1252051584.0000, content_loss=874709472.6562, style loss=355730888.6719, tv loss=21611160.0000\n",
            "L-BFGS | iteration: 812, total loss=1251860608.0000, content_loss=874689941.4062, style loss=355562636.7188, tv loss=21608048.0000\n",
            "L-BFGS | iteration: 813, total loss=1251674880.0000, content_loss=874550585.9375, style loss=355522500.0000, tv loss=21601750.0000\n",
            "L-BFGS | iteration: 814, total loss=1251505408.0000, content_loss=874492089.8438, style loss=355414189.4531, tv loss=21599084.0000\n",
            "L-BFGS | iteration: 815, total loss=1251352960.0000, content_loss=874421972.6562, style loss=355336201.1719, tv loss=21594800.0000\n",
            "L-BFGS | iteration: 816, total loss=1251127168.0000, content_loss=874332910.1562, style loss=355206064.4531, tv loss=21588148.0000\n",
            "L-BFGS | iteration: 817, total loss=1250932224.0000, content_loss=874344238.2812, style loss=355003798.8281, tv loss=21584168.0000\n",
            "L-BFGS | iteration: 818, total loss=1250749824.0000, content_loss=874297558.5938, style loss=354872841.7969, tv loss=21579348.0000\n",
            "L-BFGS | iteration: 819, total loss=1250574720.0000, content_loss=874295605.4688, style loss=354703095.7031, tv loss=21576110.0000\n",
            "L-BFGS | iteration: 820, total loss=1250438912.0000, content_loss=874095703.1250, style loss=354774873.0469, tv loss=21568442.0000\n",
            "L-BFGS | iteration: 821, total loss=1250268672.0000, content_loss=874069531.2500, style loss=354629853.5156, tv loss=21569304.0000\n",
            "L-BFGS | iteration: 822, total loss=1250117888.0000, content_loss=874001074.2188, style loss=354550283.2031, tv loss=21566412.0000\n",
            "L-BFGS | iteration: 823, total loss=1249974400.0000, content_loss=873894628.9062, style loss=354516796.8750, tv loss=21563032.0000\n",
            "L-BFGS | iteration: 824, total loss=1249805440.0000, content_loss=873783593.7500, style loss=354463593.7500, tv loss=21558218.0000\n",
            "L-BFGS | iteration: 825, total loss=1249627776.0000, content_loss=873740429.6875, style loss=354332343.7500, tv loss=21555012.0000\n",
            "L-BFGS | iteration: 826, total loss=1249445632.0000, content_loss=873706347.6562, style loss=354189375.0000, tv loss=21549802.0000\n",
            "L-BFGS | iteration: 827, total loss=1249278080.0000, content_loss=873648535.1562, style loss=354084990.2344, tv loss=21544600.0000\n",
            "L-BFGS | iteration: 828, total loss=1249118848.0000, content_loss=873663867.1875, style loss=353913837.8906, tv loss=21541170.0000\n",
            "L-BFGS | iteration: 829, total loss=1248983168.0000, content_loss=873615429.6875, style loss=353830722.6562, tv loss=21537068.0000\n",
            "L-BFGS | iteration: 830, total loss=1248801920.0000, content_loss=873595312.5000, style loss=353673281.2500, tv loss=21533326.0000\n",
            "L-BFGS | iteration: 831, total loss=1248626560.0000, content_loss=873500585.9375, style loss=353599599.6094, tv loss=21526382.0000\n",
            "L-BFGS | iteration: 832, total loss=1248467456.0000, content_loss=873514062.5000, style loss=353426308.5938, tv loss=21527066.0000\n",
            "L-BFGS | iteration: 833, total loss=1248315520.0000, content_loss=873487304.6875, style loss=353305078.1250, tv loss=21523088.0000\n",
            "L-BFGS | iteration: 834, total loss=1248171264.0000, content_loss=873418652.3438, style loss=353232128.9062, tv loss=21520486.0000\n",
            "L-BFGS | iteration: 835, total loss=1247958272.0000, content_loss=873280761.7188, style loss=353162636.7188, tv loss=21514926.0000\n",
            "L-BFGS | iteration: 836, total loss=1247760000.0000, content_loss=873093359.3750, style loss=353159208.9844, tv loss=21507496.0000\n",
            "L-BFGS | iteration: 837, total loss=1247578752.0000, content_loss=873152734.3750, style loss=352921230.4688, tv loss=21504874.0000\n",
            "L-BFGS | iteration: 838, total loss=1247461248.0000, content_loss=873114550.7812, style loss=352844736.3281, tv loss=21501906.0000\n",
            "L-BFGS | iteration: 839, total loss=1247255296.0000, content_loss=873028808.5938, style loss=352729277.3438, tv loss=21497244.0000\n",
            "L-BFGS | iteration: 840, total loss=1247055232.0000, content_loss=872936621.0938, style loss=352628203.1250, tv loss=21490470.0000\n",
            "L-BFGS | iteration: 841, total loss=1246855040.0000, content_loss=872944238.2812, style loss=352423066.4062, tv loss=21487742.0000\n",
            "L-BFGS | iteration: 842, total loss=1246689280.0000, content_loss=872872363.2812, style loss=352332949.2188, tv loss=21483916.0000\n",
            "L-BFGS | iteration: 843, total loss=1246506624.0000, content_loss=872697949.2188, style loss=352330224.6094, tv loss=21478476.0000\n",
            "L-BFGS | iteration: 844, total loss=1246357888.0000, content_loss=872645800.7812, style loss=352237734.3750, tv loss=21474274.0000\n",
            "L-BFGS | iteration: 845, total loss=1246217856.0000, content_loss=872572851.5625, style loss=352174746.0938, tv loss=21470180.0000\n",
            "L-BFGS | iteration: 846, total loss=1246085504.0000, content_loss=872525781.2500, style loss=352090869.1406, tv loss=21468858.0000\n",
            "L-BFGS | iteration: 847, total loss=1245965952.0000, content_loss=872473535.1562, style loss=352029257.8125, tv loss=21463202.0000\n",
            "L-BFGS | iteration: 848, total loss=1245852032.0000, content_loss=872490234.3750, style loss=351899414.0625, tv loss=21462448.0000\n",
            "L-BFGS | iteration: 849, total loss=1245673344.0000, content_loss=872425488.2812, style loss=351788613.2812, tv loss=21459300.0000\n",
            "L-BFGS | iteration: 850, total loss=1245496960.0000, content_loss=872373925.7812, style loss=351667939.4531, tv loss=21455132.0000\n",
            "L-BFGS | iteration: 851, total loss=1245274368.0000, content_loss=872187695.3125, style loss=351639755.8594, tv loss=21446884.0000\n",
            "L-BFGS | iteration: 852, total loss=1245129216.0000, content_loss=872255664.0625, style loss=351429990.2344, tv loss=21443640.0000\n",
            "L-BFGS | iteration: 853, total loss=1245017472.0000, content_loss=872215429.6875, style loss=351360380.8594, tv loss=21441640.0000\n",
            "L-BFGS | iteration: 854, total loss=1244864512.0000, content_loss=872124609.3750, style loss=351304013.6719, tv loss=21435938.0000\n",
            "L-BFGS | iteration: 855, total loss=1244739840.0000, content_loss=872124511.7188, style loss=351181113.2812, tv loss=21434220.0000\n",
            "L-BFGS | iteration: 856, total loss=1244556032.0000, content_loss=872112988.2812, style loss=351016318.3594, tv loss=21426736.0000\n",
            "L-BFGS | iteration: 857, total loss=1244368384.0000, content_loss=872072851.5625, style loss=350870712.8906, tv loss=21424956.0000\n",
            "L-BFGS | iteration: 858, total loss=1244210688.0000, content_loss=872065917.9688, style loss=350724228.5156, tv loss=21420572.0000\n",
            "L-BFGS | iteration: 859, total loss=1244097536.0000, content_loss=871999902.3438, style loss=350680458.9844, tv loss=21417152.0000\n",
            "L-BFGS | iteration: 860, total loss=1243948288.0000, content_loss=871880566.4062, style loss=350654707.0312, tv loss=21413016.0000\n",
            "L-BFGS | iteration: 861, total loss=1243805952.0000, content_loss=871874511.7188, style loss=350524394.5312, tv loss=21407154.0000\n",
            "L-BFGS | iteration: 862, total loss=1243654528.0000, content_loss=871848828.1250, style loss=350399970.7031, tv loss=21405716.0000\n",
            "L-BFGS | iteration: 863, total loss=1243529472.0000, content_loss=871796093.7500, style loss=350330507.8125, tv loss=21402818.0000\n",
            "L-BFGS | iteration: 864, total loss=1243389952.0000, content_loss=871786230.4688, style loss=350202246.0938, tv loss=21401432.0000\n",
            "L-BFGS | iteration: 865, total loss=1243227776.0000, content_loss=871598535.1562, style loss=350235117.1875, tv loss=21394192.0000\n",
            "L-BFGS | iteration: 866, total loss=1243063936.0000, content_loss=871524218.7500, style loss=350146376.9531, tv loss=21393294.0000\n",
            "L-BFGS | iteration: 867, total loss=1242913024.0000, content_loss=871443847.6562, style loss=350079873.0469, tv loss=21389272.0000\n",
            "L-BFGS | iteration: 868, total loss=1242754432.0000, content_loss=871367382.8125, style loss=350001328.1250, tv loss=21385690.0000\n",
            "L-BFGS | iteration: 869, total loss=1242595968.0000, content_loss=871283789.0625, style loss=349932041.0156, tv loss=21380282.0000\n",
            "L-BFGS | iteration: 870, total loss=1242447872.0000, content_loss=871267187.5000, style loss=349804599.6094, tv loss=21376052.0000\n",
            "L-BFGS | iteration: 871, total loss=1242297344.0000, content_loss=871214648.4375, style loss=349709208.9844, tv loss=21373480.0000\n",
            "L-BFGS | iteration: 872, total loss=1242129792.0000, content_loss=871133203.1250, style loss=349628759.7656, tv loss=21367760.0000\n",
            "L-BFGS | iteration: 873, total loss=1241942784.0000, content_loss=871129003.9062, style loss=349448642.5781, tv loss=21365084.0000\n",
            "L-BFGS | iteration: 874, total loss=1241771904.0000, content_loss=871011425.7812, style loss=349402089.8438, tv loss=21358292.0000\n",
            "L-BFGS | iteration: 875, total loss=1241611264.0000, content_loss=871040625.0000, style loss=349213447.2656, tv loss=21357144.0000\n",
            "L-BFGS | iteration: 876, total loss=1241468416.0000, content_loss=870987500.0000, style loss=349127578.1250, tv loss=21353312.0000\n",
            "L-BFGS | iteration: 877, total loss=1241268224.0000, content_loss=870906054.6875, style loss=349015869.1406, tv loss=21346252.0000\n",
            "L-BFGS | iteration: 878, total loss=1241108736.0000, content_loss=870795507.8125, style loss=348970869.1406, tv loss=21342298.0000\n",
            "L-BFGS | iteration: 879, total loss=1240931072.0000, content_loss=870664648.4375, style loss=348928037.1094, tv loss=21338346.0000\n",
            "L-BFGS | iteration: 880, total loss=1240835072.0000, content_loss=870637402.3438, style loss=348860361.3281, tv loss=21337306.0000\n",
            "L-BFGS | iteration: 881, total loss=1240693120.0000, content_loss=870571875.0000, style loss=348788115.2344, tv loss=21333132.0000\n",
            "L-BFGS | iteration: 882, total loss=1240523904.0000, content_loss=870558593.7500, style loss=348636035.1562, tv loss=21329310.0000\n",
            "L-BFGS | iteration: 883, total loss=1240357248.0000, content_loss=870449121.0938, style loss=348584501.9531, tv loss=21323690.0000\n",
            "L-BFGS | iteration: 884, total loss=1240217728.0000, content_loss=870427246.0938, style loss=348468339.8438, tv loss=21322166.0000\n",
            "L-BFGS | iteration: 885, total loss=1240064896.0000, content_loss=870382910.1562, style loss=348365419.9219, tv loss=21316576.0000\n",
            "L-BFGS | iteration: 886, total loss=1239911680.0000, content_loss=870366210.9375, style loss=348230712.8906, tv loss=21314866.0000\n",
            "L-BFGS | iteration: 887, total loss=1239784448.0000, content_loss=870281250.0000, style loss=348193330.0781, tv loss=21309832.0000\n",
            "L-BFGS | iteration: 888, total loss=1239660416.0000, content_loss=870255664.0625, style loss=348097236.3281, tv loss=21307536.0000\n",
            "L-BFGS | iteration: 889, total loss=1239523200.0000, content_loss=870190429.6875, style loss=348029355.4688, tv loss=21303476.0000\n",
            "L-BFGS | iteration: 890, total loss=1239363456.0000, content_loss=870123632.8125, style loss=347941142.5781, tv loss=21298742.0000\n",
            "L-BFGS | iteration: 891, total loss=1239231104.0000, content_loss=870133984.3750, style loss=347800751.9531, tv loss=21296440.0000\n",
            "L-BFGS | iteration: 892, total loss=1239109376.0000, content_loss=870075390.6250, style loss=347741162.1094, tv loss=21292772.0000\n",
            "L-BFGS | iteration: 893, total loss=1238933760.0000, content_loss=869990722.6562, style loss=347656259.7656, tv loss=21286772.0000\n",
            "L-BFGS | iteration: 894, total loss=1238753792.0000, content_loss=869878222.6562, style loss=347593330.0781, tv loss=21282316.0000\n",
            "L-BFGS | iteration: 895, total loss=1238590720.0000, content_loss=869824902.3438, style loss=347486250.0000, tv loss=21279574.0000\n",
            "L-BFGS | iteration: 896, total loss=1238492160.0000, content_loss=869791503.9062, style loss=347424111.3281, tv loss=21276562.0000\n",
            "L-BFGS | iteration: 897, total loss=1238365696.0000, content_loss=869818554.6875, style loss=347271650.3906, tv loss=21275548.0000\n",
            "L-BFGS | iteration: 898, total loss=1238228736.0000, content_loss=869800488.2812, style loss=347159033.2031, tv loss=21269252.0000\n",
            "L-BFGS | iteration: 899, total loss=1238084224.0000, content_loss=869820214.8438, style loss=346996113.2812, tv loss=21267842.0000\n",
            "L-BFGS | iteration: 900, total loss=1237924352.0000, content_loss=869735546.8750, style loss=346925830.0781, tv loss=21262920.0000\n",
            "L-BFGS | iteration: 901, total loss=1237801472.0000, content_loss=869713183.5938, style loss=346827011.7188, tv loss=21261264.0000\n",
            "L-BFGS | iteration: 902, total loss=1237672960.0000, content_loss=869629003.9062, style loss=346786757.8125, tv loss=21257168.0000\n",
            "L-BFGS | iteration: 903, total loss=1237488384.0000, content_loss=869568164.0625, style loss=346669599.6094, tv loss=21250524.0000\n",
            "L-BFGS | iteration: 904, total loss=1237368448.0000, content_loss=869473730.4688, style loss=346648330.0781, tv loss=21246342.0000\n",
            "L-BFGS | iteration: 905, total loss=1237240192.0000, content_loss=869521484.3750, style loss=346471699.2188, tv loss=21246972.0000\n",
            "L-BFGS | iteration: 906, total loss=1237116800.0000, content_loss=869510351.5625, style loss=346363740.2344, tv loss=21242712.0000\n",
            "L-BFGS | iteration: 907, total loss=1237019904.0000, content_loss=869510839.8438, style loss=346267822.2656, tv loss=21241170.0000\n",
            "L-BFGS | iteration: 908, total loss=1236843136.0000, content_loss=869446582.0312, style loss=346161445.3125, tv loss=21235104.0000\n",
            "L-BFGS | iteration: 909, total loss=1236649984.0000, content_loss=869349902.3438, style loss=346070390.6250, tv loss=21229638.0000\n",
            "L-BFGS | iteration: 910, total loss=1236514176.0000, content_loss=869285839.8438, style loss=346003447.2656, tv loss=21224858.0000\n",
            "L-BFGS | iteration: 911, total loss=1236392448.0000, content_loss=869285058.5938, style loss=345883886.7188, tv loss=21223604.0000\n",
            "L-BFGS | iteration: 912, total loss=1236294016.0000, content_loss=869228515.6250, style loss=345843896.4844, tv loss=21221484.0000\n",
            "L-BFGS | iteration: 913, total loss=1236186496.0000, content_loss=869151464.8438, style loss=345816064.4531, tv loss=21218926.0000\n",
            "L-BFGS | iteration: 914, total loss=1236025216.0000, content_loss=869062109.3750, style loss=345749472.6562, tv loss=21213714.0000\n",
            "L-BFGS | iteration: 915, total loss=1235874560.0000, content_loss=868954394.5312, style loss=345710214.8438, tv loss=21210002.0000\n",
            "L-BFGS | iteration: 916, total loss=1235720576.0000, content_loss=868984570.3125, style loss=345530214.8438, tv loss=21205818.0000\n",
            "L-BFGS | iteration: 917, total loss=1235621632.0000, content_loss=868986328.1250, style loss=345429375.0000, tv loss=21206062.0000\n",
            "L-BFGS | iteration: 918, total loss=1235499520.0000, content_loss=868883886.7188, style loss=345415283.2031, tv loss=21200344.0000\n",
            "L-BFGS | iteration: 919, total loss=1235370880.0000, content_loss=868825585.9375, style loss=345347138.6719, tv loss=21198200.0000\n",
            "L-BFGS | iteration: 920, total loss=1235195648.0000, content_loss=868694921.8750, style loss=345308115.2344, tv loss=21192616.0000\n",
            "L-BFGS | iteration: 921, total loss=1235047808.0000, content_loss=868634472.6562, style loss=345224501.9531, tv loss=21188708.0000\n",
            "L-BFGS | iteration: 922, total loss=1234929664.0000, content_loss=868612890.6250, style loss=345130839.8438, tv loss=21185950.0000\n",
            "L-BFGS | iteration: 923, total loss=1234815616.0000, content_loss=868617871.0938, style loss=345014824.2188, tv loss=21182920.0000\n",
            "L-BFGS | iteration: 924, total loss=1234669312.0000, content_loss=868584179.6875, style loss=344906337.8906, tv loss=21178728.0000\n",
            "L-BFGS | iteration: 925, total loss=1234474240.0000, content_loss=868476464.8438, style loss=344824101.5625, tv loss=21173600.0000\n",
            "L-BFGS | iteration: 926, total loss=1234307584.0000, content_loss=868512500.0000, style loss=344626962.8906, tv loss=21168116.0000\n",
            "L-BFGS | iteration: 927, total loss=1234173568.0000, content_loss=868373437.5000, style loss=344633964.8438, tv loss=21166256.0000\n",
            "L-BFGS | iteration: 928, total loss=1234086528.0000, content_loss=868326757.8125, style loss=344594882.8125, tv loss=21164942.0000\n",
            "L-BFGS | iteration: 929, total loss=1233936896.0000, content_loss=868268164.0625, style loss=344508251.9531, tv loss=21160464.0000\n",
            "L-BFGS | iteration: 930, total loss=1233785216.0000, content_loss=868236035.1562, style loss=344392529.2969, tv loss=21156572.0000\n",
            "L-BFGS | iteration: 931, total loss=1233650560.0000, content_loss=868262109.3750, style loss=344236787.1094, tv loss=21151676.0000\n",
            "L-BFGS | iteration: 932, total loss=1233550464.0000, content_loss=868273437.5000, style loss=344126367.1875, tv loss=21150780.0000\n",
            "L-BFGS | iteration: 933, total loss=1233462144.0000, content_loss=868151171.8750, style loss=344166416.0156, tv loss=21144570.0000\n",
            "L-BFGS | iteration: 934, total loss=1233338112.0000, content_loss=868162988.2812, style loss=344029277.3438, tv loss=21145802.0000\n",
            "L-BFGS | iteration: 935, total loss=1233232384.0000, content_loss=868126074.2188, style loss=343962539.0625, tv loss=21143772.0000\n",
            "L-BFGS | iteration: 936, total loss=1233098368.0000, content_loss=868045214.8438, style loss=343913935.5469, tv loss=21139316.0000\n",
            "L-BFGS | iteration: 937, total loss=1232968448.0000, content_loss=868029003.9062, style loss=343803369.1406, tv loss=21136140.0000\n",
            "L-BFGS | iteration: 938, total loss=1232777856.0000, content_loss=867974511.7188, style loss=343673408.2031, tv loss=21129836.0000\n",
            "L-BFGS | iteration: 939, total loss=1232635904.0000, content_loss=868065136.7188, style loss=343443398.4375, tv loss=21127364.0000\n",
            "L-BFGS | iteration: 940, total loss=1232529280.0000, content_loss=867975488.2812, style loss=343428662.1094, tv loss=21125084.0000\n",
            "L-BFGS | iteration: 941, total loss=1232408960.0000, content_loss=867856152.3438, style loss=343431210.9375, tv loss=21121646.0000\n",
            "L-BFGS | iteration: 942, total loss=1232293120.0000, content_loss=867786523.4375, style loss=343387968.7500, tv loss=21118760.0000\n",
            "L-BFGS | iteration: 943, total loss=1232115072.0000, content_loss=867689941.4062, style loss=343312119.1406, tv loss=21112920.0000\n",
            "L-BFGS | iteration: 944, total loss=1231953792.0000, content_loss=867635351.5625, style loss=343208759.7656, tv loss=21109608.0000\n",
            "L-BFGS | iteration: 945, total loss=1231817600.0000, content_loss=867612890.6250, style loss=343099335.9375, tv loss=21105362.0000\n",
            "L-BFGS | iteration: 946, total loss=1231714304.0000, content_loss=867616308.5938, style loss=342993720.7031, tv loss=21104316.0000\n",
            "L-BFGS | iteration: 947, total loss=1231595520.0000, content_loss=867544335.9375, style loss=342952031.2500, tv loss=21099168.0000\n",
            "L-BFGS | iteration: 948, total loss=1231490304.0000, content_loss=867506640.6250, style loss=342884296.8750, tv loss=21099436.0000\n",
            "L-BFGS | iteration: 949, total loss=1231362816.0000, content_loss=867453125.0000, style loss=342813369.1406, tv loss=21096322.0000\n",
            "L-BFGS | iteration: 950, total loss=1231227392.0000, content_loss=867370703.1250, style loss=342764970.7031, tv loss=21091690.0000\n",
            "L-BFGS | iteration: 951, total loss=1231107968.0000, content_loss=867306445.3125, style loss=342712705.0781, tv loss=21088900.0000\n",
            "L-BFGS | iteration: 952, total loss=1230961792.0000, content_loss=867291308.5938, style loss=342584824.2188, tv loss=21085586.0000\n",
            "L-BFGS | iteration: 953, total loss=1230820736.0000, content_loss=867173437.5000, style loss=342566074.2188, tv loss=21081182.0000\n",
            "L-BFGS | iteration: 954, total loss=1230678784.0000, content_loss=867175097.6562, style loss=342425009.7656, tv loss=21078688.0000\n",
            "L-BFGS | iteration: 955, total loss=1230546944.0000, content_loss=867076464.8438, style loss=342396064.4531, tv loss=21074388.0000\n",
            "L-BFGS | iteration: 956, total loss=1230403840.0000, content_loss=867059863.2812, style loss=342272607.4219, tv loss=21071370.0000\n",
            "L-BFGS | iteration: 957, total loss=1230289024.0000, content_loss=866972070.3125, style loss=342248906.2500, tv loss=21067984.0000\n",
            "L-BFGS | iteration: 958, total loss=1230170624.0000, content_loss=866979492.1875, style loss=342126474.6094, tv loss=21064760.0000\n",
            "L-BFGS | iteration: 959, total loss=1230050688.0000, content_loss=866983007.8125, style loss=342004072.2656, tv loss=21063620.0000\n",
            "L-BFGS | iteration: 960, total loss=1229928064.0000, content_loss=866906542.9688, style loss=341962675.7812, tv loss=21058834.0000\n",
            "L-BFGS | iteration: 961, total loss=1229785600.0000, content_loss=866844628.9062, style loss=341885214.8438, tv loss=21055750.0000\n",
            "L-BFGS | iteration: 962, total loss=1229618944.0000, content_loss=866741210.9375, style loss=341826386.7188, tv loss=21051372.0000\n",
            "L-BFGS | iteration: 963, total loss=1229454336.0000, content_loss=866730371.0938, style loss=341678232.4219, tv loss=21045824.0000\n",
            "L-BFGS | iteration: 964, total loss=1229342720.0000, content_loss=866646093.7500, style loss=341651894.5312, tv loss=21044772.0000\n",
            "L-BFGS | iteration: 965, total loss=1229247872.0000, content_loss=866646386.7188, style loss=341558671.8750, tv loss=21042764.0000\n",
            "L-BFGS | iteration: 966, total loss=1229136000.0000, content_loss=866673925.7812, style loss=341422529.2969, tv loss=21039430.0000\n",
            "L-BFGS | iteration: 967, total loss=1229028480.0000, content_loss=866654980.4688, style loss=341336337.8906, tv loss=21037220.0000\n",
            "L-BFGS | iteration: 968, total loss=1228882176.0000, content_loss=866576269.5312, style loss=341274814.4531, tv loss=21031170.0000\n",
            "L-BFGS | iteration: 969, total loss=1228770176.0000, content_loss=866534667.9688, style loss=341204589.8438, tv loss=21030876.0000\n",
            "L-BFGS | iteration: 970, total loss=1228674944.0000, content_loss=866475195.3125, style loss=341171748.0469, tv loss=21027968.0000\n",
            "L-BFGS | iteration: 971, total loss=1228535552.0000, content_loss=866400488.2812, style loss=341111103.5156, tv loss=21023948.0000\n",
            "L-BFGS | iteration: 972, total loss=1228432768.0000, content_loss=866296093.7500, style loss=341116259.7656, tv loss=21020442.0000\n",
            "L-BFGS | iteration: 973, total loss=1228307072.0000, content_loss=866231738.2812, style loss=341057343.7500, tv loss=21018012.0000\n",
            "L-BFGS | iteration: 974, total loss=1228174080.0000, content_loss=866129589.8438, style loss=341030830.0781, tv loss=21013640.0000\n",
            "L-BFGS | iteration: 975, total loss=1228044288.0000, content_loss=866113183.5938, style loss=340919970.7031, tv loss=21011236.0000\n",
            "L-BFGS | iteration: 976, total loss=1227933952.0000, content_loss=866122167.9688, style loss=340802167.9688, tv loss=21009698.0000\n",
            "L-BFGS | iteration: 977, total loss=1227817344.0000, content_loss=866116308.5938, style loss=340695292.9688, tv loss=21005664.0000\n",
            "L-BFGS | iteration: 978, total loss=1227667584.0000, content_loss=866133984.3750, style loss=340531552.7344, tv loss=21001944.0000\n",
            "L-BFGS | iteration: 979, total loss=1227513344.0000, content_loss=866073730.4688, style loss=340442548.8281, tv loss=20997088.0000\n",
            "L-BFGS | iteration: 980, total loss=1227373440.0000, content_loss=865965820.3125, style loss=340413486.3281, tv loss=20994180.0000\n",
            "L-BFGS | iteration: 981, total loss=1227249536.0000, content_loss=865901562.5000, style loss=340356972.6562, tv loss=20991034.0000\n",
            "L-BFGS | iteration: 982, total loss=1227118848.0000, content_loss=865795996.0938, style loss=340335292.9688, tv loss=20987616.0000\n",
            "L-BFGS | iteration: 983, total loss=1226984832.0000, content_loss=865732812.5000, style loss=340267851.5625, tv loss=20984252.0000\n",
            "L-BFGS | iteration: 984, total loss=1226852352.0000, content_loss=865781738.2812, style loss=340089433.5938, tv loss=20981104.0000\n",
            "L-BFGS | iteration: 985, total loss=1226734080.0000, content_loss=865743652.3438, style loss=340012441.4062, tv loss=20977880.0000\n",
            "L-BFGS | iteration: 986, total loss=1226602880.0000, content_loss=865772949.2188, style loss=339854765.6250, tv loss=20975196.0000\n",
            "L-BFGS | iteration: 987, total loss=1226507392.0000, content_loss=865665429.6875, style loss=339872812.5000, tv loss=20969256.0000\n",
            "L-BFGS | iteration: 988, total loss=1226378112.0000, content_loss=865610644.5312, style loss=339797548.8281, tv loss=20969996.0000\n",
            "L-BFGS | iteration: 989, total loss=1226269568.0000, content_loss=865522656.2500, style loss=339779355.4688, tv loss=20967568.0000\n",
            "L-BFGS | iteration: 990, total loss=1226161408.0000, content_loss=865439746.0938, style loss=339756357.4219, tv loss=20965416.0000\n",
            "L-BFGS | iteration: 991, total loss=1226031488.0000, content_loss=865277636.7188, style loss=339793916.0156, tv loss=20959864.0000\n",
            "L-BFGS | iteration: 992, total loss=1225917440.0000, content_loss=865268652.3438, style loss=339690673.8281, tv loss=20958044.0000\n",
            "L-BFGS | iteration: 993, total loss=1225823232.0000, content_loss=865258691.4062, style loss=339608935.5469, tv loss=20955700.0000\n",
            "L-BFGS | iteration: 994, total loss=1225681536.0000, content_loss=865221582.0312, style loss=339508798.8281, tv loss=20951106.0000\n",
            "L-BFGS | iteration: 995, total loss=1225537152.0000, content_loss=865217871.0938, style loss=339371455.0781, tv loss=20947828.0000\n",
            "L-BFGS | iteration: 996, total loss=1225398272.0000, content_loss=865156347.6562, style loss=339297861.3281, tv loss=20944164.0000\n",
            "L-BFGS | iteration: 997, total loss=1225292288.0000, content_loss=865132519.5312, style loss=339216796.8750, tv loss=20943004.0000\n",
            "L-BFGS | iteration: 998, total loss=1225192704.0000, content_loss=865103710.9375, style loss=339148564.4531, tv loss=20940370.0000\n",
            "L-BFGS | iteration: 999, total loss=1225076224.0000, content_loss=865069335.9375, style loss=339069228.5156, tv loss=20937768.0000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'img_format'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-3a750da9ef73>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Run the neural style transfer function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'neural_style_transfer'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mresults_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneural_style_transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimization_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Function 'neural_style_transfer' is not defined.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-10ae190bd4e4>\u001b[0m in \u001b[0;36mneural_style_transfer\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdump_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    442\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0m\u001b[1;32m    445\u001b[0m                         \u001b[0mobj_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgtd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# evaluate objective and gradient using initial step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mls_func_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mgtd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-10ae190bd4e4>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'L-BFGS | iteration: {cnt:03}, total loss={total_loss.item():12.4f}, content_loss={config[\"content_weight\"] * content_loss.item():12.4f}, style loss={config[\"style_weight\"] * style_loss.item():12.4f}, tv loss={config[\"tv_weight\"] * tv_loss.item():12.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0msave_and_maybe_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizing_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdump_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_iterations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_display\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mcnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-1b82eac843b3>\u001b[0m in \u001b[0;36msave_and_maybe_display\u001b[0;34m(optimizing_img, dump_path, config, img_id, num_of_iterations, should_display)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimg_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnum_of_iterations\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msaving_freq\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mimg_id\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msaving_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mimg_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img_format'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mout_img_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_format\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mimg_format\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msaving_freq\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mgenerate_out_img_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mdump_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'img_format'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DC6XLPw2nir7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}